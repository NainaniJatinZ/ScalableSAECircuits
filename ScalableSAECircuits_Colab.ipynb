{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from functools import partial\n",
    "import einops\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ") \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from functools import lru_cache\n",
    "from typing import TypedDict, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44205f9617f94ffca8aad396bd872f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-9b\", device=device, cache_dir=hf_cache) \n",
    "\n",
    "pad_token_id = model.tokenizer.pad_token_id\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "layers= [7, 14, 21, 40]\n",
    "l0s = [92, 67, 129, 125]\n",
    "saes = [SAE.from_pretrained(release=\"gemma-scope-9b-pt-res\", sae_id=f\"layer_{layers[i]}/width_16k/average_l0_{l0s[i]}\", device=device)[0] for i in range(len(layers))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions and classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cuda():\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "\n",
    "def clear_memory():\n",
    "   for sae in saes:\n",
    "      for param in sae.parameters():\n",
    "         param.grad = None\n",
    "      for param in sae.mask.parameters():\n",
    "         param.grad = None\n",
    "\n",
    "   for param in model.parameters():\n",
    "      param.grad = None\n",
    "   cleanup_cuda()\n",
    "\n",
    "class SAEMasks(nn.Module):\n",
    "    def __init__(self, hook_points, masks):\n",
    "        super().__init__()\n",
    "        self.hook_points = hook_points  # list of strings\n",
    "        self.masks = masks\n",
    "\n",
    "    def forward(self, x, sae_hook_point, mean_ablation=None):\n",
    "        index = self.hook_points.index(sae_hook_point)\n",
    "        mask = self.masks[index]\n",
    "        censored_activations = torch.ones_like(x)\n",
    "        if mean_ablation is not None:\n",
    "            censored_activations = censored_activations * mean_ablation\n",
    "        else:\n",
    "            censored_activations = censored_activations * 0\n",
    "        \n",
    "        diff_to_x = x - censored_activations\n",
    "        return censored_activations + diff_to_x * mask\n",
    "\n",
    "    def print_mask_statistics(self):\n",
    "        \"\"\"\n",
    "        Prints statistics about each binary mask:\n",
    "          - total number of elements (latents)\n",
    "          - total number of 'on' latents (mask == 1)\n",
    "          - average on-latents per token\n",
    "            * If shape == [latent_dim], there's effectively 1 token\n",
    "            * If shape == [seq, latent_dim], it's 'sum of on-latents / seq'\n",
    "        \"\"\"\n",
    "        for i, mask in enumerate(self.masks):\n",
    "            shape = list(mask.shape)\n",
    "            total_latents = mask.numel()\n",
    "            total_on = mask.sum().item()  # number of 1's in the mask\n",
    "\n",
    "            # Average on-latents per token depends on dimensions\n",
    "            if len(shape) == 1:\n",
    "                # e.g., shape == [latent_dim]\n",
    "                avg_on_per_token = total_on  # only one token\n",
    "            elif len(shape) == 2:\n",
    "                # e.g., shape == [seq, latent_dim]\n",
    "                seq_len = shape[0]\n",
    "                avg_on_per_token = total_on / seq_len if seq_len > 0 else 0\n",
    "            else:\n",
    "                # If there's more than 2 dims, adapt as needed;\n",
    "                # we'll just define \"token\" as the first dimension.\n",
    "                seq_len = shape[0]\n",
    "                avg_on_per_token = total_on / seq_len if seq_len > 0 else 0\n",
    "\n",
    "            print(f\"Statistics for mask '{self.hook_points[i]}':\")\n",
    "            print(f\"  - Shape: {shape}\")\n",
    "            print(f\"  - Total latents: {total_latents}\")\n",
    "            print(f\"  - Latents ON (mask=1): {int(total_on)}\")\n",
    "            print(f\"  - Average ON per token: {avg_on_per_token:.4f}\\n\")\n",
    "\n",
    "    def save(self, save_dir, file_name=\"sae_masks.pt\"):\n",
    "        \"\"\"\n",
    "        Saves hook_points and masks to a single file (file_name) within save_dir.\n",
    "        If you want multiple mask sets in the same directory, call save() with\n",
    "        different file_name values. The directory is created if it does not exist.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        save_path = os.path.join(save_dir, file_name)\n",
    "        checkpoint = {\n",
    "            \"hook_points\": self.hook_points,\n",
    "            \"masks\": self.masks\n",
    "        }\n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(f\"SAEMasks saved to {save_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, load_dir, file_name=\"sae_masks.pt\"):\n",
    "        \"\"\"\n",
    "        Loads hook_points and masks from a single file (file_name) within load_dir,\n",
    "        returning an instance of SAEMasks. If you stored multiple mask sets in the\n",
    "        directory, specify the file_name to load the correct one.\n",
    "        \"\"\"\n",
    "        load_path = os.path.join(load_dir, file_name)\n",
    "        if not os.path.isfile(load_path):\n",
    "            raise FileNotFoundError(f\"No saved SAEMasks found at {load_path}\")\n",
    "\n",
    "        checkpoint = torch.load(load_path)\n",
    "        hook_points = checkpoint[\"hook_points\"]\n",
    "        masks = checkpoint[\"masks\"]\n",
    "\n",
    "        instance = cls(hook_points=hook_points, masks=masks)\n",
    "        print(f\"SAEMasks loaded from {load_path}\")\n",
    "        return instance\n",
    "    def get_num_latents(self):\n",
    "        num_latents = 0\n",
    "        for mask in self.masks:\n",
    "            num_latents += (mask>0).sum().item()\n",
    "        return num_latents\n",
    "\n",
    "class SparseMask(nn.Module):\n",
    "    def __init__(self, shape, l1, seq_len=None, distinct_l1=0):\n",
    "        super().__init__()\n",
    "        if seq_len is not None:\n",
    "            self.mask = nn.Parameter(torch.ones(seq_len, shape))\n",
    "        else:\n",
    "            self.mask = nn.Parameter(torch.ones(shape))\n",
    "        self.l1 = l1\n",
    "        self.distinct_l1 = distinct_l1\n",
    "        self.max_temp = torch.tensor(1000.0)\n",
    "        self.sparsity_loss = None\n",
    "        self.ratio_trained = 1\n",
    "        self.temperature = 1\n",
    "        self.distinct_sparsity_loss = 0\n",
    "\n",
    "\n",
    "    def forward(self, x, binary=False, mean_ablation=None):\n",
    "        if binary:\n",
    "            # binary mask, 0 if negative, 1 if positive\n",
    "            binarized = (self.mask > 0).float()\n",
    "            if mean_ablation is None:\n",
    "                return x * binarized\n",
    "            else:\n",
    "                diff = x - mean_ablation\n",
    "                return diff * binarized + mean_ablation\n",
    "            \n",
    "\n",
    "        self.temperature = self.max_temp ** self.ratio_trained\n",
    "        mask = torch.sigmoid(self.mask * self.temperature)\n",
    "        # mask = self.mask\n",
    "        self.sparsity_loss = torch.abs(mask).sum() * self.l1\n",
    "        # print(\"hello\", torch.abs(mask).sum()) \n",
    "        # if len(mask.shape) == 2:\n",
    "        #     self.distinct_sparsity_loss = torch.abs(mask).max(dim=0).values.sum() * self.distinct_l1\n",
    "\n",
    "        if mean_ablation is None:\n",
    "            return x * mask\n",
    "        else:\n",
    "            diff = x - mean_ablation\n",
    "            return diff * mask + mean_ablation\n",
    "\n",
    "# for sae in saes:\n",
    "#     sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=65)\n",
    "\n",
    "class IGMask(nn.Module):\n",
    "    # igscores is seq x num_sae_latents\n",
    "    def __init__(self, ig_scores):\n",
    "        super().__init__()\n",
    "        self.ig_scores = ig_scores\n",
    "\n",
    "    def forward(self, x, threshold, mean_ablation = None):\n",
    "        censored_activations = torch.ones_like(x)\n",
    "        if mean_ablation != None:\n",
    "            censored_activations = censored_activations * mean_ablation\n",
    "        else:\n",
    "            censored_activations = censored_activations * 0\n",
    "\n",
    "        mask = (self.ig_scores.abs() > threshold).float()\n",
    "        \n",
    "        diff_to_x = x - censored_activations\n",
    "        return censored_activations + diff_to_x * mask\n",
    "    \n",
    "    def get_threshold_info(self, threshold):\n",
    "        mask = (self.ig_scores.abs() > threshold).float()\n",
    "\n",
    "        total_latents = mask.sum()\n",
    "        avg_latents_per_tok = mask.sum()/mask.shape[0]\n",
    "        latents_per_tok = mask.sum(dim=-1)\n",
    "        return {\"total_latents\":total_latents,\n",
    "                \"avg_latents_per_tok\":avg_latents_per_tok,\n",
    "                \"latents_per_tok\":latents_per_tok}\n",
    "    \n",
    "    def get_binarized_mask(self, threshold):\n",
    "        return (self.ig_scores.abs()>threshold).float()\n",
    "    \n",
    "def refresh_class():\n",
    "    for sae in saes:\n",
    "        if hasattr(sae, 'igmask'):\n",
    "            sae.igmask = IGMask(sae.igmask.ig_scores)\n",
    "\n",
    "try:\n",
    "    refresh_class()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "refresh_class()\n",
    "\n",
    "def produce_ig_binary_masks(threshold=0.01):\n",
    "    hook_points = []\n",
    "    masks = []\n",
    "\n",
    "    for sae in saes:\n",
    "        hook_point = sae.cfg.hook_name\n",
    "        mask = sae.igmask.get_binarized_mask(threshold=threshold)\n",
    "        hook_points.append(hook_point)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return SAEMasks(\n",
    "        hook_points=hook_points,\n",
    "        masks=masks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = model.tokenizer.bos_token_id\n",
    "\n",
    "def build_sae_hook_fn(\n",
    "    # Core components\n",
    "    sae,\n",
    "    sequence,\n",
    "    \n",
    "    # Masking options\n",
    "    circuit_mask: Optional[SAEMasks] = None,\n",
    "    use_mask=False,\n",
    "    binarize_mask=False,\n",
    "    mean_mask=False,\n",
    "    ig_mask_threshold=None,\n",
    "    \n",
    "    # Caching behavior\n",
    "    cache_sae_grads=False,\n",
    "    cache_masked_activations=False,\n",
    "    cache_sae_activations=False,\n",
    "    \n",
    "    # Ablation options\n",
    "    mean_ablate=False,  # Controls mean ablation of the SAE\n",
    "    fake_activations=False,  # Controls whether to use fake activations\n",
    "    ):    # make the mask for the sequence\n",
    "    mask = torch.ones_like(sequence, dtype=torch.bool)\n",
    "    # mask[sequence == pad_token_id] = False\n",
    "    mask[sequence == bos_token_id] = False # where mask is false, keep original\n",
    "    def sae_hook(value, hook):\n",
    "        # print(f\"sae {sae.cfg.hook_name} running at layer {hook.layer()}\")\n",
    "        feature_acts = sae.encode(value)\n",
    "        feature_acts = feature_acts * mask.unsqueeze(-1)\n",
    "        if fake_activations != False and sae.cfg.hook_layer == fake_activations[0]:\n",
    "            feature_acts = fake_activations[1]\n",
    "        if cache_sae_grads:\n",
    "            raise NotImplementedError(\"torch is confusing\")\n",
    "            sae.feature_acts = feature_acts.requires_grad_(True)\n",
    "            sae.feature_acts.retain_grad()\n",
    "        \n",
    "        if cache_sae_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        \n",
    "        # Learned Binary Masking\n",
    "        if use_mask:\n",
    "            if mean_mask:\n",
    "                # apply the mask, with mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                # apply the mask, without mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask)\n",
    "\n",
    "        # IG Masking\n",
    "        if ig_mask_threshold != None:\n",
    "            # apply the ig mask\n",
    "            if mean_mask:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold)\n",
    "\n",
    "                \n",
    "        if circuit_mask is not None:\n",
    "            hook_point = sae.cfg.hook_name\n",
    "            if mean_mask==True:\n",
    "                feature_acts = circuit_mask(feature_acts, hook_point, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                feature_acts = circuit_mask(feature_acts, hook_point)\n",
    "            \n",
    "        if cache_masked_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        if mean_ablate:\n",
    "            feature_acts = sae.mean_ablation\n",
    "\n",
    "        out = sae.decode(feature_acts)\n",
    "        # choose out or value based on the mask\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(value)\n",
    "        value = torch.where(mask_expanded, out, value)\n",
    "        return value\n",
    "    return sae_hook\n",
    "\n",
    "def build_hooks_list(sequence,\n",
    "                    cache_sae_activations=False,\n",
    "                    cache_sae_grads=False,\n",
    "                    circuit_mask=None,\n",
    "                    use_mask=False,\n",
    "                    binarize_mask=False,\n",
    "                    mean_mask=False,\n",
    "                    cache_masked_activations=False,\n",
    "                    mean_ablate=False,\n",
    "                    fake_activations: Tuple[int, torch.Tensor] = False,\n",
    "                    ig_mask_threshold=None,\n",
    "                    ):\n",
    "    hooks = []\n",
    "    for sae in saes:\n",
    "        hooks.append(\n",
    "            (\n",
    "            sae.cfg.hook_name,\n",
    "            build_sae_hook_fn(sae, sequence, cache_sae_grads=cache_sae_grads, circuit_mask=circuit_mask, use_mask=use_mask, binarize_mask=binarize_mask, cache_masked_activations=cache_masked_activations, cache_sae_activations=cache_sae_activations, mean_mask=mean_mask, mean_ablate=mean_ablate, fake_activations=fake_activations, ig_mask_threshold=ig_mask_threshold),\n",
    "            )\n",
    "        )\n",
    "    return hooks \n",
    "\n",
    "def build_sae_logitfn(**kwargs):\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, **kwargs)\n",
    "            )\n",
    "    return logitfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "N = 6\n",
    "K = 1\n",
    "file_path = f'data/codereason/index/data_len{N}_digit{K}.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "example_length = 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12080\n",
      "torch.Size([3000, 29]) torch.Size([3000, 29])\n",
      "torch.Size([187, 16, 29]) torch.Size([187, 16, 29]) torch.Size([187, 16]) torch.Size([187, 16])\n"
     ]
    }
   ],
   "source": [
    "clean_data = []\n",
    "corr_data = []\n",
    "clean_labels = []\n",
    "corr_labels = []\n",
    "for entry in data:\n",
    "    clean_len = len(model.tokenizer(entry['clean_prefix']).input_ids)\n",
    "    corr_len = len(model.tokenizer(entry['patch_prefix']).input_ids)\n",
    "    if clean_len == corr_len == 29:\n",
    "        clean_data.append(entry['clean_prefix'])\n",
    "        corr_data.append(entry['patch_prefix'])\n",
    "        clean_labels.append(entry['clean_answer'])\n",
    "        corr_labels.append(entry['patch_answer'])\n",
    "print(len(clean_data))\n",
    "\n",
    "N = 3000\n",
    "clean_tokens = model.to_tokens(clean_data[:N])\n",
    "corr_tokens = model.to_tokens(corr_data[:N])\n",
    "clean_label_tokens = model.to_tokens(clean_labels[:N], prepend_bos=False).squeeze(-1)\n",
    "corr_label_tokens = model.to_tokens(corr_labels[:N], prepend_bos=False).squeeze(-1)\n",
    "print(clean_tokens.shape, corr_tokens.shape)\n",
    "\n",
    "def logit_diff_fn(logits, clean_labels, corr_labels, token_wise=False):\n",
    "    clean_logits = logits[torch.arange(logits.shape[0]), -1, clean_labels]\n",
    "    corr_logits = logits[torch.arange(logits.shape[0]), -1, corr_labels]\n",
    "    return (clean_logits - corr_logits).mean() if not token_wise else (clean_logits - corr_logits)\n",
    "\n",
    "batch_size = 16 \n",
    "clean_tokens = clean_tokens[:batch_size*(len(clean_tokens)//batch_size)]\n",
    "corr_tokens = corr_tokens[:batch_size*(len(corr_tokens)//batch_size)]\n",
    "clean_label_tokens = clean_label_tokens[:batch_size*(len(clean_label_tokens)//batch_size)]\n",
    "corr_label_tokens = corr_label_tokens[:batch_size*(len(corr_label_tokens)//batch_size)]\n",
    "\n",
    "clean_tokens = clean_tokens.reshape(-1, batch_size, clean_tokens.shape[-1])\n",
    "corr_tokens = corr_tokens.reshape(-1, batch_size, corr_tokens.shape[-1])\n",
    "clean_label_tokens = clean_label_tokens.reshape(-1, batch_size)\n",
    "corr_label_tokens = corr_label_tokens.reshape(-1, batch_size)\n",
    "\n",
    "print(clean_tokens.shape, corr_tokens.shape, clean_label_tokens.shape, corr_label_tokens.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2678, device='cuda:0')\n",
      "tensor(4.3395, device='cuda:0')\n",
      "tensor(3.6911, device='cuda:0')\n",
      "tensor(4.4798, device='cuda:0')\n",
      "tensor(4.2827, device='cuda:0')\n",
      "tensor(4.2606, device='cuda:0')\n",
      "tensor(4.3051, device='cuda:0')\n",
      "tensor(3.9882, device='cuda:0')\n",
      "tensor(4.0821, device='cuda:0')\n",
      "tensor(3.9572, device='cuda:0')\n",
      "Average LD:  4.165408134460449\n"
     ]
    }
   ],
   "source": [
    "use_mask = False \n",
    "mean_mask = False\n",
    "avg_logit_diff = 0\n",
    "cleanup_cuda()\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        logits = model.run_with_hooks(\n",
    "            clean_tokens[i], \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(clean_tokens[i], use_mask=use_mask, mean_mask=mean_mask)\n",
    "            )\n",
    "        ld = logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i])\n",
    "        print(ld)\n",
    "        avg_logit_diff += ld\n",
    "        del logits\n",
    "        cleanup_cuda()\n",
    "avg_logit_diff = (avg_logit_diff / 10).item()\n",
    "print(\"Average LD: \", avg_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Accum Progress:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Accum Progress: 100%|██████████| 640/640 [02:37<00:00,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "for sae in saes:\n",
    "    sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length).to(device)\n",
    "\n",
    "def running_mean_tensor(old_mean, new_value, n):\n",
    "    return old_mean + (new_value - old_mean) / n\n",
    "\n",
    "def get_sae_means(mean_tokens, total_batches, batch_size, per_token_mask=False):\n",
    "    for sae in saes:\n",
    "        sae.mean_ablation = torch.zeros(sae.cfg.d_sae).float().to(device)\n",
    "    \n",
    "    with tqdm(total=total_batches*batch_size, desc=\"Mean Accum Progress\") as pbar:\n",
    "        for i in range(total_batches):\n",
    "            for j in range(batch_size):\n",
    "                with torch.no_grad():\n",
    "                    _ = model.run_with_hooks(\n",
    "                        mean_tokens[i, j], \n",
    "                        return_type=\"logits\", \n",
    "                        fwd_hooks=build_hooks_list(mean_tokens[i, j], cache_sae_activations=True)\n",
    "                        )\n",
    "                    for sae in saes:\n",
    "                        sae.mean_ablation = running_mean_tensor(sae.mean_ablation, sae.feature_acts, i+1)\n",
    "                    cleanup_cuda()\n",
    "                pbar.update(1)\n",
    "\n",
    "            if i >= total_batches:\n",
    "                break\n",
    "\n",
    "get_sae_means(corr_tokens, 40, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "Average LD:  tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "avg_mean_diff = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(3):\n",
    "        logits = model.run_with_hooks(\n",
    "            clean_tokens[i], \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(clean_tokens[i], mean_ablate=True)\n",
    "            )\n",
    "        ld = logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i])\n",
    "        print(ld)\n",
    "        avg_mean_diff += ld\n",
    "        del logits\n",
    "        cleanup_cuda()\n",
    "print(\"Average LD: \", avg_mean_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Mask Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import signal\n",
    "class KeyboardInterruptBlocker:\n",
    "    def __enter__(self):\n",
    "        # Block SIGINT and store old mask\n",
    "        self.old_mask = signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGINT})\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore old mask (unblock SIGINT)\n",
    "        signal.pthread_sigmask(signal.SIG_SETMASK, self.old_mask)\n",
    "\n",
    "class Range:\n",
    "    def __init__(self, *args):\n",
    "        # Support for range(start, stop, step) or range(stop)\n",
    "        self.args = args\n",
    "\n",
    "        # Validate input like the built-in range does\n",
    "        if len(self.args) not in {1, 2, 3}:\n",
    "            raise TypeError(f\"Range expected at most 3 arguments, got {len(self.args)}\")\n",
    "        \n",
    "        self.range = __builtins__.range(*self.args)  # Create the range object\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in self.range:\n",
    "            try:\n",
    "                with KeyboardInterruptBlocker():\n",
    "                    yield i\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Keyboard interrupt received. Exiting iteration.\")\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.range)\n",
    "\n",
    "def do_training_run(token_dataset, labels_dataset, corr_labels_dataset, sparsity_multiplier, task, example_length=6, loss_function='ce', per_token_mask=False, use_mask=True, mean_mask=False, portion_of_data = 0.5, distinct_sparsity_multiplier=0 ):\n",
    "\n",
    "    def logitfn(tokens, use_mask, mean_mask):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask)\n",
    "            )\n",
    "        return logits\n",
    "\n",
    "    def forward_pass(batch, clean_label_tokens, corr_label_tokens, logitfn, ratio_trained=1, loss_function='ce'):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        tokens = batch\n",
    "        logits = logitfn(tokens, use_mask, mean_mask)\n",
    "        model_logits = logitfn(tokens, False, False)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        if loss_function == 'ce':\n",
    "            loss = F.cross_entropy(last_token_logits, clean_label_tokens)\n",
    "        elif loss_function == 'logit_diff':\n",
    "            fwd_logit_diff = logit_diff_fn(logits, clean_label_tokens, corr_label_tokens)\n",
    "            model_logit_diff = logit_diff_fn(model_logits, clean_label_tokens, corr_label_tokens)\n",
    "            loss = torch.abs(model_logit_diff - fwd_logit_diff)\n",
    "        \n",
    "        del model_logits, logits\n",
    "        cleanup_cuda()\n",
    "\n",
    "        sparsity_loss = 0\n",
    "        distinct_sparsity_loss = 0\n",
    "        # if per_token_mask:\n",
    "        #     distinct_sparsity_loss = 0\n",
    "        for sae in saes:\n",
    "            sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "            # if per_token_mask:\n",
    "            #     distinct_sparsity_loss = distinct_sparsity_loss + sae.mask.distinct_sparsity_loss\n",
    "        \n",
    "        sparsity_loss = sparsity_loss / len(saes)\n",
    "        distinct_sparsity_loss = distinct_sparsity_loss / len(saes)\n",
    "\n",
    "        return loss, sparsity_loss, distinct_sparsity_loss\n",
    "\n",
    "    print(\"doing a run with sparsity multiplier\", sparsity_multiplier)\n",
    "    all_optimized_params = []\n",
    "    config = {\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 1,\n",
    "        \"total_steps\": token_dataset.shape[0]*portion_of_data,\n",
    "        \"sparsity_multiplier\": sparsity_multiplier\n",
    "    }\n",
    "\n",
    "    for sae in saes:\n",
    "        if per_token_mask:\n",
    "            sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length).to(device)\n",
    "        else:\n",
    "            sae.mask = SparseMask(sae.cfg.d_sae, 1.0).to(device)\n",
    "        all_optimized_params.extend(list(sae.mask.parameters()))\n",
    "        sae.mask.max_temp = torch.tensor(200.0)\n",
    "    \n",
    "    wandb.init(project=\"sae circuits\", config=config)\n",
    "    optimizer = optim.Adam(all_optimized_params, lr=config[\"learning_rate\"])\n",
    "    total_steps = config[\"total_steps\"] #*20 #*config[\"batch_size\"]\n",
    "    # epochs = 20\n",
    "    with tqdm(total=total_steps*1.1, desc=\"Training Progress\") as pbar:\n",
    "        # for epoch in range(epochs):\n",
    "        for i, (x, y, z) in enumerate(zip(token_dataset, labels_dataset, corr_labels_dataset)):\n",
    "            with KeyboardInterruptBlocker():\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Calculate ratio trained\n",
    "                ratio_trained =  i / total_steps*1.1\n",
    "                \n",
    "                # Update mask ratio for each SAE\n",
    "                for sae in saes:\n",
    "                    sae.mask.ratio_trained = ratio_trained\n",
    "                \n",
    "                # Forward pass with updated ratio_trained\n",
    "                loss, sparsity_loss, _ = forward_pass(x, y, z, logitfn, ratio_trained=ratio_trained, loss_function=loss_function)\n",
    "                # if per_token_mask:\n",
    "                #     sparsity_loss = sparsity_loss / example_length\n",
    "                # print(\"sp loss\", sparsity_loss)\n",
    "                # print(\"sae 0 sp loss\", saes[0].mask.sparsity_loss)\n",
    "                # print(\"sae 1 sp loss\", saes[1].mask.sparsity_loss)\n",
    "                # print(\"sae 2 sp loss\", saes[2].mask.sparsity_loss)\n",
    "                # print(\"sae 3 sp loss\", saes[3].mask.sparsity_loss)\n",
    "                \n",
    "                avg_nonzero_elements = sparsity_loss\n",
    "                # avg_distinct_nonzero_elements = distinct_sparsity_loss\n",
    "                    \n",
    "                sparsity_loss = sparsity_loss * config[\"sparsity_multiplier\"] #+ distinct_sparsity_loss * distinct_sparsity_multiplier\n",
    "                total_loss = sparsity_loss + loss\n",
    "                infodict  = {\"Step\": i, \"Progress\": ratio_trained, \"Avg Nonzero Elements\": avg_nonzero_elements.item(), \"Task Loss\": loss.item(), \"Sparsity Loss\": sparsity_loss.item(), \"temperature\": saes[0].mask.temperature} # \"avg distinct lat/sae\":avg_distinct_nonzero_elements.item()\n",
    "                wandb.log(infodict)\n",
    "                # break\n",
    "                # Backward pass and optimizer step\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update tqdm bar with relevant metrics\n",
    "                pbar.set_postfix(infodict)\n",
    "                \n",
    "                # Update the tqdm progress bar\n",
    "                pbar.update(1)\n",
    "                # break\n",
    "                if i >= total_steps*1.1:\n",
    "                    break\n",
    "    wandb.finish()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for sae in saes:\n",
    "        for param in sae.parameters():\n",
    "            param.grad = None\n",
    "        for param in sae.mask.parameters():\n",
    "            param.grad = None\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ### EVAL ###\n",
    "    def masked_logit_fn(tokens):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask, binarize_mask=True)\n",
    "            )\n",
    "        return logits\n",
    "\n",
    "    def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        tokens = batch\n",
    "        logits = logitfn(tokens)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        loss = F.cross_entropy(last_token_logits, labels)\n",
    "        return loss\n",
    "    \n",
    "    def eval_logit_diff(num_batches, batch, clean_labels, corr_labels, logitfn, ratio_trained=10):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        avg_ld = 0\n",
    "        for i in range(num_batches):\n",
    "            tokens = batch[-i]\n",
    "            logits = logitfn(tokens)\n",
    "            ld = logit_diff_fn(logits, clean_labels[-i], corr_labels[-i])\n",
    "            avg_ld += ld\n",
    "            del logits\n",
    "            cleanup_cuda()\n",
    "        return (avg_ld / num_batches).item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = eval_ce_loss(token_dataset[-1], labels_dataset[-1], masked_logit_fn)\n",
    "        print(\"CE loss:\", loss)\n",
    "        cleanup_cuda()\n",
    "        logit_diff = eval_logit_diff(10, token_dataset, labels_dataset, corr_labels_dataset, masked_logit_fn)\n",
    "        print(\"Logit Diff:\", logit_diff)\n",
    "        cleanup_cuda()\n",
    "    \n",
    "    mask_dict = {}\n",
    "\n",
    "    total_density = 0\n",
    "    for sae in saes:\n",
    "        mask_dict[sae.cfg.hook_name] = torch.where(sae.mask.mask > 0)[1].tolist()   # rob thinks .view(-1) needed here\n",
    "        total_density += (sae.mask.mask > 0).sum().item()\n",
    "    mask_dict[\"total_density\"] = total_density\n",
    "    mask_dict['avg_density'] = total_density / len(saes)\n",
    "\n",
    "    if per_token_mask:\n",
    "        print(\"total # latents in circuit: \", total_density)\n",
    "    print(\"avg density\", mask_dict['avg_density'])\n",
    "\n",
    "    save_path = f\"masks/{task}/{loss_function}_{str(sparsity_multiplier)}_run/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    mask_dict['ce_loss'] = loss.item()\n",
    "    mask_dict['logit_diff'] = logit_diff\n",
    "    faithfulness = logit_diff / avg_logit_diff\n",
    "    mask_dict['faithfulness'] = faithfulness\n",
    "    \n",
    "    for idx, sae in enumerate(saes):\n",
    "        mask_path = f\"sae_mask_{idx}.pt\"\n",
    "        torch.save(sae.mask.state_dict(), os.path.join(save_path,mask_path))\n",
    "        print(f\"Saved mask for SAE {idx} to {mask_path}\")\n",
    "\n",
    "    json.dump(mask_dict, open(os.path.join(save_path,f\"{str(sparsity_multiplier)}_run.json\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.01,\n",
       " 0.020495238095238094,\n",
       " 0.04474285714285713,\n",
       " 0.08274285714285712,\n",
       " 0.13449523809523808]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholds = []\n",
    "modify_fn=lambda x: x**2\n",
    "start_threshold = 0.01\n",
    "end_threshold = 0.2\n",
    "n_runs = 5\n",
    "delta = (end_threshold - start_threshold) / n_runs\n",
    "def linear_map(x):\n",
    "        mod_start = modify_fn(start_threshold)\n",
    "        mod_end = modify_fn(end_threshold)\n",
    "        return (x - mod_start) / (mod_end - mod_start) * (end_threshold - start_threshold) + start_threshold\n",
    "    \n",
    "mf = lambda x: linear_map(modify_fn(x))\n",
    "for i in range(n_runs):\n",
    "    thresholds.append(\n",
    "        mf(start_threshold + i*delta)\n",
    "        )\n",
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in thresholds:\n",
    "    do_training_run(token_dataset=clean_tokens, labels_dataset= clean_label_tokens, corr_labels_dataset=corr_label_tokens, sparsity_multiplier=i, task=f'codereason/index/len6_digit1', example_length=example_length, loss_function=\"logit_diff\", per_token_mask=True, use_mask=True, mean_mask=True, portion_of_data=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated Gradients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def get_ig_effect_batch(sae_id, token_dataset, corr_token_dataset, labels_dataset, corr_labels_dataset):\n",
    "    # ig_batch_size = 4\n",
    "\n",
    "    def logitfn_cache(tokens):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask, cache_sae_activations=True)\n",
    "            )\n",
    "        cached_activations = [sae.feature_acts for sae in saes]\n",
    "        return logits, cached_activations\n",
    "\n",
    "    def sae_continue_cache_logitfn(tokens, sae_idx, cache, **kwargs):\n",
    "        logits = model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, fake_activations=(sae_idx, cache),**kwargs)\n",
    "            )\n",
    "        # logitfn = build_sae_logitfn(fake_activations=(sae_idx, cache), **kwargs)\n",
    "        return logits\n",
    "    \n",
    "    # run a forward pass on the correct and error examples\n",
    "    with torch.no_grad():\n",
    "        _, clean_activations = logitfn_cache(token_dataset)\n",
    "        _, corr_activations = logitfn_cache(corr_token_dataset)\n",
    "\n",
    "\n",
    "    sae = saes[sae_id]\n",
    "    sae_clean_acts = clean_activations[sae_id]\n",
    "    sae_corr_acts = corr_activations[sae_id]\n",
    "\n",
    "    steps = 10\n",
    "    ratios = [i/steps for i in range(steps)] # we skip the last one b/c we are integrating, obv\n",
    "        \n",
    "    effects = []\n",
    "    for ratio in ratios:\n",
    "        interpolation = (sae_clean_acts * (1-ratio) + sae_corr_acts * (ratio)).requires_grad_(True)\n",
    "        interpolation.retain_grad()\n",
    "        interpolated_out = sae_continue_cache_logitfn(\n",
    "            token_dataset, sae.cfg.hook_layer, interpolation,\n",
    "        )\n",
    "        answer_logits = interpolated_out[..., -1, :] # get the logits of the last tokens\n",
    "        # get the logprob on the inner most dimension\n",
    "        answer_logprobs = F.log_softmax(answer_logits, dim=-1)\n",
    "\n",
    "        clean_logprobs = answer_logprobs[..., torch.arange(answer_logprobs.shape[-2]), labels_dataset]\n",
    "        corr_logprobs = answer_logprobs[..., torch.arange(answer_logprobs.shape[-2]), corr_labels_dataset]\n",
    "        metric = torch.sum(clean_logprobs - corr_logprobs)\n",
    "        metric.backward()\n",
    "\n",
    "        counterfactual_delta = sae_clean_acts - sae_corr_acts\n",
    "        \n",
    "        effect = (interpolation.grad * counterfactual_delta).mean(dim=0)\n",
    "\n",
    "        effects.append(effect)\n",
    "\n",
    "    effects = torch.stack(effects)\n",
    "    clear_memory()\n",
    "    return effects.mean(dim=0)\n",
    "\n",
    "def get_sae_ig_effect(sae_id, num_batches, clean_tokens, corr_tokens, clean_label_tokens, corr_label_tokens):\n",
    "    effect_batches = []\n",
    "    for i in tqdm(range(0, num_batches)):\n",
    "        clean_batch = clean_tokens[i]  \n",
    "        corr_batch = corr_tokens[i]\n",
    "        clean_label_batch = clean_label_tokens[i]\n",
    "        corr_label_batch = corr_label_tokens[i]\n",
    "        effect_batches.append(get_ig_effect_batch(sae_id, clean_batch, corr_batch, clean_label_batch, corr_label_batch))\n",
    "    return torch.stack(effect_batches).mean(dim=0)\n",
    "\n",
    "def get_all_sae_ig_effects(num_batches, clean_tokens, corr_tokens, clean_label_tokens, corr_label_tokens):\n",
    "    for i in range(len(saes)):\n",
    "        ig_effect = get_sae_ig_effect(i, num_batches, clean_tokens, corr_tokens, clean_label_tokens, corr_label_tokens)\n",
    "        saes[i].igmask = IGMask(ig_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_sae_ig_effects(25, clean_tokens, corr_tokens, clean_label_tokens, corr_label_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_igmask(igmask, file_path):\n",
    "    \"\"\"Save the IGMask instance to a file.\"\"\"\n",
    "    if not isinstance(igmask, IGMask):\n",
    "        raise ValueError(\"The object to save must be an instance of IGMask.\")\n",
    "\n",
    "    # Save the ig_scores and any additional parameters if needed\n",
    "    state_dict = {\n",
    "        'ig_scores': igmask.ig_scores\n",
    "    }\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    # Save the state dictionary using PyTorch\n",
    "    torch.save(state_dict, file_path)\n",
    "\n",
    "for sae in saes:\n",
    "    save_igmask(sae.igmask, f\"masks/codereason/index/len6_digit1/igmask/igmask_{sae.cfg.hook_layer}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.00001\n",
    "sum = 0\n",
    "for sae in saes:\n",
    "    print(\"layer:\", sae.cfg.hook_layer)\n",
    "    latents_in_sae = sae.igmask.get_threshold_info(threshold)['total_latents'].item()\n",
    "    sum += latents_in_sae\n",
    "    print(latents_in_sae)\n",
    "\n",
    "print(\"total\", sum)\n",
    "\n",
    "print(\"=\"*40)\n",
    "with torch.no_grad():\n",
    "    ig_logits = model.run_with_hooks(\n",
    "            clean_tokens[-1], \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(clean_tokens[-1], ig_mask_threshold=threshold, mean_mask=True)\n",
    "            )\n",
    "    ld = logit_diff_fn(ig_logits, clean_label_tokens[-1], corr_label_tokens[-1])\n",
    "    print(ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison:\n",
    "\n",
    "def get_faithfulness(clean_tokens, clean_label_tokens, corr_label_tokens, mask, modelsae_ld):\n",
    "    avg_ig_ld = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(10):\n",
    "            ig_logits = model.run_with_hooks(\n",
    "                    clean_tokens[-i], \n",
    "                    return_type=\"logits\", \n",
    "                    fwd_hooks=build_hooks_list(clean_tokens[-i], circuit_mask=mask, mean_mask=True)\n",
    "                    )\n",
    "            ld = logit_diff_fn(ig_logits, clean_label_tokens[-i], corr_label_tokens[-i])\n",
    "            avg_ig_ld += ld\n",
    "            del ig_logits\n",
    "            cleanup_cuda()\n",
    "    avg_ig = (avg_ig_ld / 10).item()\n",
    "    faithfulness_ratio = avg_ig/modelsae_ld\n",
    "    return faithfulness_ratio\n",
    "\n",
    "def get_faithfulness_frontier(mask_producer_fn, start_threshold, end_threshold, clean_tokens, clean_label_tokens, corr_label_tokens, modelsae_ld, steps = 10, modify_fn = lambda x: x):\n",
    "    thresholds = []\n",
    "    delta = (end_threshold - start_threshold) / steps\n",
    "\n",
    "    # apply a linear function such that the linear_map(modify_fn(start_threshold)) = start_threshold\n",
    "    # and linear_map(modify_fn(end_threshold)) = end_threshold\n",
    "\n",
    "    def linear_map(x):\n",
    "        mod_start = modify_fn(start_threshold)\n",
    "        mod_end = modify_fn(end_threshold)\n",
    "        return (x - mod_start) / (mod_end - mod_start) * (end_threshold - start_threshold) + start_threshold\n",
    "    \n",
    "    mf = lambda x: linear_map(modify_fn(x))\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(steps):\n",
    "        thresholds.append(\n",
    "            mf(start_threshold + i*delta)\n",
    "            )\n",
    "    \n",
    "    faithfulness_scores = []\n",
    "    for threshold in tqdm(thresholds):\n",
    "        mask = mask_producer_fn(threshold=threshold)\n",
    "        num_latents = mask.get_num_latents()\n",
    "        faithfulness_ratio = get_faithfulness(clean_tokens, clean_label_tokens, corr_label_tokens, mask, modelsae_ld\n",
    "        )\n",
    "        faithfulness_scores.append([num_latents, faithfulness_ratio])\n",
    "    return faithfulness_scores        \n",
    "\n",
    "ig_faithfulness_frontier = get_faithfulness_frontier(produce_ig_binary_masks, 0.00005, 0.05, clean_tokens, clean_label_tokens, corr_label_tokens, avg_logit_diff, steps=10, modify_fn=lambda x: x**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circuit Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# circuit loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = []\n",
    "modify_fn=lambda x: x**2\n",
    "# thresholds = []\n",
    "# modify_fn=lambda x: x**2\n",
    "start_threshold = 0.01\n",
    "end_threshold = 0.2\n",
    "n_runs = 5\n",
    "delta = (end_threshold - start_threshold) / n_runs\n",
    "def linear_map(x):\n",
    "        mod_start = modify_fn(start_threshold)\n",
    "        mod_end = modify_fn(end_threshold)\n",
    "        return (x - mod_start) / (mod_end - mod_start) * (end_threshold - start_threshold) + start_threshold\n",
    "    \n",
    "mf = lambda x: linear_map(modify_fn(x))\n",
    "for i in range(n_runs):\n",
    "    thresholds.append(\n",
    "        mf(start_threshold + i*delta)\n",
    "        )\n",
    "thresholds\n",
    "nnodes = []\n",
    "faithfulness = []\n",
    "for i in thresholds:\n",
    "    sparsity_multiplier = i\n",
    "    bdir = f\"masks/codereason/key/len5_digit1/logit_diff_{str(sparsity_multiplier)}_run/{str(sparsity_multiplier)}_run.json\"\n",
    "    # load the dict \n",
    "    with open(bdir, 'r') as file:\n",
    "        mask_dict = json.load(file)\n",
    "    nnodes.append(mask_dict['total_density'])\n",
    "    faithfulness.append(mask_dict['faithfulness'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faithfulness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = []\n",
    "modify_fn=lambda x: x**2\n",
    "start_threshold = 0.01\n",
    "end_threshold = 0.2\n",
    "n_runs = 5\n",
    "delta = (end_threshold - start_threshold) / n_runs\n",
    "def linear_map(x):\n",
    "        mod_start = modify_fn(start_threshold)\n",
    "        mod_end = modify_fn(end_threshold)\n",
    "        return (x - mod_start) / (mod_end - mod_start) * (end_threshold - start_threshold) + start_threshold\n",
    "    \n",
    "mf = lambda x: linear_map(modify_fn(x))\n",
    "for i in range(n_runs):\n",
    "    thresholds.append(\n",
    "        mf(start_threshold + i*delta)\n",
    "        )\n",
    "thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "thresholds = []\n",
    "modify_fn=lambda x: x**2\n",
    "# thresholds = []\n",
    "# modify_fn=lambda x: x**2\n",
    "start_threshold = 0.01\n",
    "end_threshold = 0.2\n",
    "n_runs = 5\n",
    "delta = (end_threshold - start_threshold) / n_runs\n",
    "def linear_map(x):\n",
    "        mod_start = modify_fn(start_threshold)\n",
    "        mod_end = modify_fn(end_threshold)\n",
    "        return (x - mod_start) / (mod_end - mod_start) * (end_threshold - start_threshold) + start_threshold\n",
    "    \n",
    "mf = lambda x: linear_map(modify_fn(x))\n",
    "for i in range(n_runs):\n",
    "    thresholds.append(\n",
    "        mf(start_threshold + i*delta)\n",
    "        )\n",
    "thresholds\n",
    "nnodes = []\n",
    "faithfulness = []\n",
    "for i in thresholds:\n",
    "    sparsity_multiplier = i\n",
    "    bdir = f\"masks/codereason/key/len5_digit1/logit_diff_{str(sparsity_multiplier)}_run/{str(sparsity_multiplier)}_run.json\"\n",
    "    # load the dict \n",
    "    with open(bdir, 'r') as file:\n",
    "        mask_dict = json.load(file)\n",
    "    nnodes.append(mask_dict['total_density'])\n",
    "    faithfulness.append(mask_dict['faithfulness'])\n",
    "\n",
    "\n",
    "# Unzip the tuples into separate lists for Integrated Gradients plot\n",
    "num_latents, ig_faithfulness = zip(*ig_faithfulness_frontier)\n",
    "\n",
    "# Create the combined plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot \"Integrated Gradients\"\n",
    "plt.plot(num_latents, ig_faithfulness, marker='o',linestyle='-', linewidth=2, markersize=8,  label='Integrated Gradients', color='blue')\n",
    "\n",
    "# Plot \"Masking\"\n",
    "plt.plot(nnodes, faithfulness, marker='o', linestyle='-', linewidth=2, markersize=8, label='Masking', color='orange')\n",
    "\n",
    "# Add labels, legend, and title\n",
    "plt.xlabel('Number of Nodes / Latents', fontsize=14)\n",
    "plt.ylabel('Faithfulness', fontsize=14)\n",
    "plt.title('Faithfulness Comparison: Masking vs Integrated Gradients', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(visible=True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sae in saes:\n",
    "    sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length, distinct_l1=1.0).to(device)\n",
    "\n",
    "def eval_logit_diff(batch, clean_labels, corr_labels, ratio_trained=10):\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    logits = model.run_with_hooks(\n",
    "        batch, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(batch, use_mask=True, mean_mask=True, binarize_mask=True)\n",
    "        )\n",
    "    return logit_diff_fn(logits, clean_labels, corr_labels)\n",
    "\n",
    "model_logit_diff = 0 \n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        batch_model_logit_diff = eval_logit_diff(clean_tokens[-i], clean_label_tokens[-i], corr_label_tokens[-i]).item()\n",
    "        model_logit_diff += batch_model_logit_diff\n",
    "    model_logit_diff = (model_logit_diff / 10)\n",
    "    cleanup_cuda()\n",
    "print(\"Logit Diff:\", model_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load masks before inference or further training\n",
    "sparsity_multiplier = 0.04474285714285713\n",
    "path = \"masks/codereason/key/len5_digit1\"\n",
    "for idx, sae in enumerate(saes):\n",
    "    mask_path = f\"{path}/logit_diff_{str(sparsity_multiplier)}_run/sae_mask_{idx}.pt\"\n",
    "    state_dict = torch.load(mask_path)\n",
    "    sae.mask.load_state_dict(state_dict)\n",
    "    print(f\"Loaded mask for SAE {idx} from {mask_path}\")\n",
    "\n",
    "circuit_logit_diff = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        batch_circuit_logit_diff = eval_logit_diff(clean_tokens[-i], clean_label_tokens[-i], corr_label_tokens[-i]).item()\n",
    "        circuit_logit_diff += batch_circuit_logit_diff\n",
    "        cleanup_cuda()\n",
    "    circuit_logit_diff = (circuit_logit_diff / 10)\n",
    "print(\"Logit Diff:\", circuit_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_multiplier = 0.04474285714285713\n",
    "path = \"masks/codereason/key/len5_digit1\"\n",
    "bdir = f\"{path}/logit_diff_{str(sparsity_multiplier)}_run/{str(sparsity_multiplier)}_run.json\"\n",
    "with open(bdir, 'r') as file:\n",
    "    mask_dict = json.load(file)\n",
    "mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Number of elements to remove from the last dimension\n",
    "num_remove = 5\n",
    "\n",
    "# Number of batches to process\n",
    "num_batches = 3  # Adjust this as needed\n",
    "batch_size = 16  # Batch size for processing\n",
    "\n",
    "# Remove random subsets of the mask along the [-1] dimension\n",
    "def get_indices_to_remove(mask, num_remove):\n",
    "    active_indices = (mask > 0).nonzero(as_tuple=True)[-1]  # Get indices of active elements in the last dimension\n",
    "    if len(active_indices) < num_remove:\n",
    "        raise ValueError(\"Not enough active elements to remove.\")\n",
    "    indices_to_remove = active_indices[torch.randperm(len(active_indices))[:num_remove]].to(mask.device)  # Move to the same device\n",
    "    return indices_to_remove\n",
    "\n",
    "def apply_subset_removal(mask, indices_to_remove):\n",
    "    indices_to_remove = indices_to_remove.to(mask.device)  # Ensure indices are on the same device\n",
    "    modified_mask = mask.clone()\n",
    "    modified_mask.index_fill_(-1, indices_to_remove, -1.9455)  # Set selected elements to 0\n",
    "    return modified_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_k_results = []\n",
    "for i in range(7):\n",
    "    indices_rem_list = []\n",
    "    for ind, sae in enumerate(saes):\n",
    "        # Calculate random num_remove as approximately half the length of the mask\n",
    "        mask_length = len(mask_dict[sae.cfg.hook_name])\n",
    "        num_remove = random.randint(max(1, mask_length // 2 - int(mask_length*0.5)), mask_length // 2 + int(mask_length*0.5))\n",
    "        indices_rem_list.append(get_indices_to_remove(sae.mask.mask.data, num_remove))\n",
    "    random_k_results.append({\"knockout_indices\": indices_rem_list})\n",
    "print(random_k_results)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate F(C \\ K) and F(M \\ K) for N batches\n",
    "num_batches = 3\n",
    "for rand_ind, k_results in enumerate(random_k_results):\n",
    "    for case in ['circuit', 'model']:\n",
    "        total_knock_logit_diff = 0.0\n",
    "        for batch_idx in range(num_batches):\n",
    "            sparsity_multiplier = 0.04474285714285713\n",
    "            path = \"masks/codereason/key/len5_digit1\"\n",
    "            with torch.no_grad():\n",
    "                for ind, sae in enumerate(saes):\n",
    "                    if case == 'circuit':\n",
    "                        mask_path = f\"{path}/logit_diff_{str(sparsity_multiplier)}_run/sae_mask_{ind}.pt\"\n",
    "                        state_dict = torch.load(mask_path)\n",
    "                        sae.mask.load_state_dict(state_dict)\n",
    "                    else:\n",
    "                        sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length, distinct_l1=1.0).to(device)\n",
    "                    sae.mask.mask.data = apply_subset_removal(sae.mask.mask.data, k_results['knockout_indices'][ind]).to(device)\n",
    "                logit_diff = eval_logit_diff(clean_tokens[-batch_idx], clean_label_tokens[-batch_idx], corr_label_tokens[-batch_idx]).item()\n",
    "                total_knock_logit_diff += logit_diff\n",
    "                cleanup_cuda()\n",
    "        # Average logit difference for the case\n",
    "        avg_knock_logit_diff = total_knock_logit_diff / num_batches\n",
    "        if case == 'circuit':\n",
    "            print(f\"Random set {rand_ind}, F(C \\ K): {avg_knock_logit_diff}\")\n",
    "            random_k_results[rand_ind]['F_C_K'] = avg_knock_logit_diff\n",
    "        else:\n",
    "            print(f\"Random set {rand_ind}, F(M \\ K): {avg_knock_logit_diff}\")\n",
    "            random_k_results[rand_ind]['F_M_K'] = avg_knock_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_rem_list_full = []\n",
    "for ind, sae in enumerate(saes):\n",
    "    # Get indices to remove based on the original mask\n",
    "    indices_rem_list_full.append(torch.tensor(mask_dict[f\"blocks.{layers[ind]}.hook_resid_post\"]))\n",
    "indices_rem_list_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for case in ['circuit', 'model']:\n",
    "    total_knock_logit_diff = 0.0\n",
    "    for batch_idx in range(num_batches):\n",
    "        sparsity_multiplier = 0.04474285714285713\n",
    "        path = \"masks/codereason/key/len5_digit1\"\n",
    "        with torch.no_grad():\n",
    "            for ind, sae in enumerate(saes):\n",
    "                if case == 'circuit':\n",
    "                    mask_path = f\"{path}/logit_diff_{str(sparsity_multiplier)}_run/sae_mask_{ind}.pt\"\n",
    "                    state_dict = torch.load(mask_path)\n",
    "                    sae.mask.load_state_dict(state_dict)\n",
    "                else:\n",
    "                    sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length, distinct_l1=1.0).to(device)\n",
    "                sae.mask.mask.data = apply_subset_removal(sae.mask.mask.data, indices_rem_list_full[ind]).to(device)\n",
    "            logit_diff = eval_logit_diff(clean_tokens[-batch_idx], clean_label_tokens[-batch_idx], corr_label_tokens[-batch_idx]).item()\n",
    "            total_knock_logit_diff += logit_diff\n",
    "            cleanup_cuda()\n",
    "    # Average logit difference for the case\n",
    "    avg_knock_logit_diff = total_knock_logit_diff / num_batches\n",
    "    if case == 'circuit':\n",
    "        print(f\"full_circ_knock_circ F(C \\ K): {avg_knock_logit_diff}\")\n",
    "        full_circ_knock_circ = avg_knock_logit_diff\n",
    "    else:\n",
    "        print(f\" full_circ_knock_model F(M \\ K): {avg_knock_logit_diff}\")\n",
    "        full_circ_knock_model = avg_knock_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Null Set']\n",
    "for i in range(7):\n",
    "    labels.append(f\"Random Set {i}\")\n",
    "labels.append('Full Circuit Removal')\n",
    "\n",
    "xy = [(circuit_logit_diff, model_logit_diff)]\n",
    "for rand_ind, k_results in enumerate(random_k_results):\n",
    "    xy.append((k_results['F_C_K'], k_results['F_M_K']))\n",
    "xy.append((full_circ_knock_circ, full_circ_knock_model))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Extract x and y values\n",
    "# Separate the data into two groups\n",
    "# Separate the data into two groups\n",
    "null_set_xy = xy[0]\n",
    "random_sets_xy = xy[1:-1]\n",
    "full_circ_rem = xy[-1]\n",
    "\n",
    "# Extract x and y values for linear regression\n",
    "all_x = [point[0] for point in xy[:-1]]\n",
    "all_y = [point[1] for point in xy[:-1]]\n",
    "\n",
    "# Fit a linear regression model\n",
    "regressor = LinearRegression()\n",
    "all_x_reshaped = np.array(all_x).reshape(-1, 1)  # Reshape for sklearn\n",
    "regressor.fit(all_x_reshaped, all_y)\n",
    "\n",
    "# Generate points for the fitted line\n",
    "fitted_x = np.linspace(min(all_x), max(all_x), 100)\n",
    "fitted_y = regressor.predict(fitted_x.reshape(-1, 1))\n",
    "\n",
    "# Extract x and y values for each group\n",
    "null_set_x, null_set_y = null_set_xy\n",
    "random_x = [point[0] for point in random_sets_xy]\n",
    "random_y = [point[1] for point in random_sets_xy]\n",
    "full_circ_x, full_circ_y = full_circ_rem\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Null set\n",
    "plt.scatter(null_set_x, null_set_y, color='red', label='Null Set', s=100)\n",
    "\n",
    "# Random sets\n",
    "plt.scatter(random_x, random_y, color='blue', label='Random Sets', s=100)\n",
    "\n",
    "# Full circuit removal\n",
    "plt.scatter(full_circ_x, full_circ_y, color='green', label='Full Circuit Removal', s=100)\n",
    "\n",
    "# Add reference line y = x\n",
    "x_line = [min([null_set_x] + random_x + [null_set_y] + random_y), max([null_set_x] + random_x + [null_set_y] + random_y)]\n",
    "plt.plot(x_line, x_line, linestyle=\"--\", color=\"gray\", label=\"y = x\")\n",
    "\n",
    "# Add fitted regression line\n",
    "plt.plot(fitted_x, fitted_y, linestyle=\"--\", color=\"green\", label=\"Fitted Line\")\n",
    "\n",
    "# Customize plot\n",
    "plt.title(\"Circuit Completeness Tests with Fitted Line\", fontsize=14)\n",
    "plt.xlabel(\"F(C \\\\ K)\", fontsize=12)\n",
    "plt.ylabel(\"F(M \\\\ K)\", fontsize=12)\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

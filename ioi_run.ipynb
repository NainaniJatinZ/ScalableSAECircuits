{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from functools import partial\n",
    "import einops\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ") \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from functools import lru_cache\n",
    "from typing import TypedDict, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "369b83c0b986462eb7a72c633830d24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-9b\", device=device, cache_dir=hf_cache) \n",
    "\n",
    "pad_token_id = model.tokenizer.pad_token_id\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "layers= [7, 14, 21, 40]\n",
    "l0s = [92, 67, 129, 125]\n",
    "saes = [SAE.from_pretrained(release=\"gemma-scope-9b-pt-res\", sae_id=f\"layer_{layers[i]}/width_16k/average_l0_{l0s[i]}\", device=device)[0] for i in range(len(layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cuda():\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "\n",
    "def clear_memory():\n",
    "   for sae in saes:\n",
    "      for param in sae.parameters():\n",
    "         param.grad = None\n",
    "      for param in sae.mask.parameters():\n",
    "         param.grad = None\n",
    "\n",
    "   for param in model.parameters():\n",
    "      param.grad = None\n",
    "   cleanup_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAEMasks(nn.Module):\n",
    "    def __init__(self, hook_points, masks):\n",
    "        super().__init__()\n",
    "        self.hook_points = hook_points  # list of strings\n",
    "        self.masks = masks\n",
    "\n",
    "    def forward(self, x, sae_hook_point, mean_ablation=None):\n",
    "        index = self.hook_points.index(sae_hook_point)\n",
    "        mask = self.masks[index]\n",
    "        censored_activations = torch.ones_like(x)\n",
    "        if mean_ablation is not None:\n",
    "            censored_activations = censored_activations * mean_ablation\n",
    "        else:\n",
    "            censored_activations = censored_activations * 0\n",
    "        \n",
    "        diff_to_x = x - censored_activations\n",
    "        return censored_activations + diff_to_x * mask\n",
    "\n",
    "    def print_mask_statistics(self):\n",
    "        \"\"\"\n",
    "        Prints statistics about each binary mask:\n",
    "          - total number of elements (latents)\n",
    "          - total number of 'on' latents (mask == 1)\n",
    "          - average on-latents per token\n",
    "            * If shape == [latent_dim], there's effectively 1 token\n",
    "            * If shape == [seq, latent_dim], it's 'sum of on-latents / seq'\n",
    "        \"\"\"\n",
    "        for i, mask in enumerate(self.masks):\n",
    "            shape = list(mask.shape)\n",
    "            total_latents = mask.numel()\n",
    "            total_on = mask.sum().item()  # number of 1's in the mask\n",
    "\n",
    "            # Average on-latents per token depends on dimensions\n",
    "            if len(shape) == 1:\n",
    "                # e.g., shape == [latent_dim]\n",
    "                avg_on_per_token = total_on  # only one token\n",
    "            elif len(shape) == 2:\n",
    "                # e.g., shape == [seq, latent_dim]\n",
    "                seq_len = shape[0]\n",
    "                avg_on_per_token = total_on / seq_len if seq_len > 0 else 0\n",
    "            else:\n",
    "                # If there's more than 2 dims, adapt as needed;\n",
    "                # we'll just define \"token\" as the first dimension.\n",
    "                seq_len = shape[0]\n",
    "                avg_on_per_token = total_on / seq_len if seq_len > 0 else 0\n",
    "\n",
    "            print(f\"Statistics for mask '{self.hook_points[i]}':\")\n",
    "            print(f\"  - Shape: {shape}\")\n",
    "            print(f\"  - Total latents: {total_latents}\")\n",
    "            print(f\"  - Latents ON (mask=1): {int(total_on)}\")\n",
    "            print(f\"  - Average ON per token: {avg_on_per_token:.4f}\\n\")\n",
    "\n",
    "    def save(self, save_dir, file_name=\"sae_masks.pt\"):\n",
    "        \"\"\"\n",
    "        Saves hook_points and masks to a single file (file_name) within save_dir.\n",
    "        If you want multiple mask sets in the same directory, call save() with\n",
    "        different file_name values. The directory is created if it does not exist.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "\n",
    "        save_path = os.path.join(save_dir, file_name)\n",
    "        checkpoint = {\n",
    "            \"hook_points\": self.hook_points,\n",
    "            \"masks\": self.masks\n",
    "        }\n",
    "        torch.save(checkpoint, save_path)\n",
    "        print(f\"SAEMasks saved to {save_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, load_dir, file_name=\"sae_masks.pt\"):\n",
    "        \"\"\"\n",
    "        Loads hook_points and masks from a single file (file_name) within load_dir,\n",
    "        returning an instance of SAEMasks. If you stored multiple mask sets in the\n",
    "        directory, specify the file_name to load the correct one.\n",
    "        \"\"\"\n",
    "        load_path = os.path.join(load_dir, file_name)\n",
    "        if not os.path.isfile(load_path):\n",
    "            raise FileNotFoundError(f\"No saved SAEMasks found at {load_path}\")\n",
    "\n",
    "        checkpoint = torch.load(load_path)\n",
    "        hook_points = checkpoint[\"hook_points\"]\n",
    "        masks = checkpoint[\"masks\"]\n",
    "\n",
    "        instance = cls(hook_points=hook_points, masks=masks)\n",
    "        print(f\"SAEMasks loaded from {load_path}\")\n",
    "        return instance\n",
    "    def get_num_latents(self):\n",
    "        num_latents = 0\n",
    "        for mask in self.masks:\n",
    "            num_latents += (mask>0).sum().item()\n",
    "        return num_latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMask(nn.Module):\n",
    "    def __init__(self, shape, l1, seq_len=None, distinct_l1=0):\n",
    "        super().__init__()\n",
    "        if seq_len is not None:\n",
    "            self.mask = nn.Parameter(torch.ones(seq_len, shape))\n",
    "        else:\n",
    "            self.mask = nn.Parameter(torch.ones(shape))\n",
    "        self.l1 = l1\n",
    "        self.distinct_l1 = distinct_l1\n",
    "        self.max_temp = torch.tensor(1000.0)\n",
    "        self.sparsity_loss = None\n",
    "        self.ratio_trained = 1\n",
    "        self.temperature = 1\n",
    "        self.distinct_sparsity_loss = 0\n",
    "\n",
    "\n",
    "    def forward(self, x, binary=False, mean_ablation=None):\n",
    "        if binary:\n",
    "            # binary mask, 0 if negative, 1 if positive\n",
    "            binarized = (self.mask > 0).float()\n",
    "            if mean_ablation is None:\n",
    "                return x * binarized\n",
    "            else:\n",
    "                diff = x - mean_ablation\n",
    "                return diff * binarized + mean_ablation\n",
    "            \n",
    "\n",
    "        self.temperature = self.max_temp ** self.ratio_trained\n",
    "        mask = torch.sigmoid(self.mask * self.temperature)\n",
    "        # mask = self.mask\n",
    "        self.sparsity_loss = torch.abs(mask).sum() * self.l1\n",
    "        # print(\"hello\", torch.abs(mask).sum()) \n",
    "        # if len(mask.shape) == 2:\n",
    "        #     self.distinct_sparsity_loss = torch.abs(mask).max(dim=0).values.sum() * self.distinct_l1\n",
    "\n",
    "        if mean_ablation is None:\n",
    "            return x * mask\n",
    "        else:\n",
    "            diff = x - mean_ablation\n",
    "            return diff * mask + mean_ablation\n",
    "\n",
    "# for sae in saes:\n",
    "#     sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IGMask(nn.Module):\n",
    "    # igscores is seq x num_sae_latents\n",
    "    def __init__(self, ig_scores):\n",
    "        super().__init__()\n",
    "        self.ig_scores = ig_scores\n",
    "\n",
    "    def forward(self, x, threshold, mean_ablation = None):\n",
    "        censored_activations = torch.ones_like(x)\n",
    "        if mean_ablation != None:\n",
    "            censored_activations = censored_activations * mean_ablation\n",
    "        else:\n",
    "            censored_activations = censored_activations * 0\n",
    "\n",
    "        mask = (self.ig_scores.abs() > threshold).float()\n",
    "        \n",
    "        diff_to_x = x - censored_activations\n",
    "        return censored_activations + diff_to_x * mask\n",
    "    \n",
    "    def get_threshold_info(self, threshold):\n",
    "        mask = (self.ig_scores.abs() > threshold).float()\n",
    "\n",
    "        total_latents = mask.sum()\n",
    "        avg_latents_per_tok = mask.sum()/mask.shape[0]\n",
    "        latents_per_tok = mask.sum(dim=-1)\n",
    "        return {\"total_latents\":total_latents,\n",
    "                \"avg_latents_per_tok\":avg_latents_per_tok,\n",
    "                \"latents_per_tok\":latents_per_tok}\n",
    "    \n",
    "    def get_binarized_mask(self, threshold):\n",
    "        return (self.ig_scores.abs()>threshold).float()\n",
    "    \n",
    "def refresh_class():\n",
    "    for sae in saes:\n",
    "        if hasattr(sae, 'igmask'):\n",
    "            sae.igmask = IGMask(sae.igmask.ig_scores)\n",
    "\n",
    "try:\n",
    "    refresh_class()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "refresh_class()\n",
    "\n",
    "\n",
    "\n",
    "def produce_ig_binary_masks(threshold=0.01):\n",
    "    hook_points = []\n",
    "    masks = []\n",
    "\n",
    "    for sae in saes:\n",
    "        hook_point = sae.cfg.hook_name\n",
    "        mask = sae.igmask.get_binarized_mask(threshold=threshold)\n",
    "        hook_points.append(hook_point)\n",
    "        masks.append(mask)\n",
    "    \n",
    "    return SAEMasks(\n",
    "        hook_points=hook_points,\n",
    "        masks=masks\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = model.tokenizer.bos_token_id\n",
    "\n",
    "def build_sae_hook_fn(\n",
    "    # Core components\n",
    "    sae,\n",
    "    sequence,\n",
    "    \n",
    "    # Masking options\n",
    "    circuit_mask: Optional[SAEMasks] = None,\n",
    "    use_mask=False,\n",
    "    binarize_mask=False,\n",
    "    mean_mask=False,\n",
    "    ig_mask_threshold=None,\n",
    "    \n",
    "    # Caching behavior\n",
    "    cache_sae_grads=False,\n",
    "    cache_masked_activations=False,\n",
    "    cache_sae_activations=False,\n",
    "    \n",
    "    # Ablation options\n",
    "    mean_ablate=False,  # Controls mean ablation of the SAE\n",
    "    fake_activations=False,  # Controls whether to use fake activations\n",
    "    ):    # make the mask for the sequence\n",
    "    mask = torch.ones_like(sequence, dtype=torch.bool)\n",
    "    # mask[sequence == pad_token_id] = False\n",
    "    mask[sequence == bos_token_id] = False # where mask is false, keep original\n",
    "    def sae_hook(value, hook):\n",
    "        # print(f\"sae {sae.cfg.hook_name} running at layer {hook.layer()}\")\n",
    "        feature_acts = sae.encode(value)\n",
    "        feature_acts = feature_acts * mask.unsqueeze(-1)\n",
    "        if fake_activations != False and sae.cfg.hook_layer == fake_activations[0]:\n",
    "            feature_acts = fake_activations[1]\n",
    "        if cache_sae_grads:\n",
    "            raise NotImplementedError(\"torch is confusing\")\n",
    "            sae.feature_acts = feature_acts.requires_grad_(True)\n",
    "            sae.feature_acts.retain_grad()\n",
    "        \n",
    "        if cache_sae_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Learned Binary Masking\n",
    "        if use_mask:\n",
    "            if mean_mask:\n",
    "                # apply the mask, with mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                # apply the mask, without mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask)\n",
    "\n",
    "        # IG Masking\n",
    "        if ig_mask_threshold != None:\n",
    "            # apply the ig mask\n",
    "            if mean_mask:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold)\n",
    "\n",
    "                \n",
    "        if circuit_mask is not None:\n",
    "            hook_point = sae.cfg.hook_name\n",
    "            if mean_mask==True:\n",
    "                feature_acts = circuit_mask(feature_acts, hook_point, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                feature_acts = circuit_mask(feature_acts, hook_point)\n",
    "            \n",
    "        if cache_masked_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        if mean_ablate:\n",
    "            feature_acts = sae.mean_ablation\n",
    "\n",
    "        out = sae.decode(feature_acts)\n",
    "        # choose out or value based on the mask\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(value)\n",
    "        value = torch.where(mask_expanded, out, value)\n",
    "        return value\n",
    "    return sae_hook\n",
    "\n",
    "\n",
    "    # def sae_hook_ablate(value, hook):\n",
    "    # feature_acts = sae.encode(value)\n",
    "    # # feature_acts[:, :, topsae_attr_indices] = 0\n",
    "    # out = sae.decode(feature_acts)\n",
    "    # return out\n",
    "\n",
    "\n",
    "def build_hooks_list(sequence,\n",
    "                    cache_sae_activations=False,\n",
    "                    cache_sae_grads=False,\n",
    "                    circuit_mask=None,\n",
    "                    use_mask=False,\n",
    "                    binarize_mask=False,\n",
    "                    mean_mask=False,\n",
    "                    cache_masked_activations=False,\n",
    "                    mean_ablate=False,\n",
    "                    fake_activations: Tuple[int, torch.Tensor] = False,\n",
    "                    ig_mask_threshold=None,\n",
    "                    ):\n",
    "    hooks = []\n",
    "    # blocks.0.hook_resid_pre\n",
    "    # # fake hook that adds zero so gradients propagate through the model\n",
    "    # param = nn.Parameter(torch.tensor(0.0, requires_grad=True))\n",
    "    # hooks.append(\n",
    "    #     (\n",
    "    #         \"blocks.0.hook_resid_pre\",\n",
    "    #         lambda value, hook: value + param,\n",
    "    #     )\n",
    "    # )\n",
    "    for sae in saes:\n",
    "        hooks.append(\n",
    "            (\n",
    "            sae.cfg.hook_name,\n",
    "            build_sae_hook_fn(sae, sequence, cache_sae_grads=cache_sae_grads, circuit_mask=circuit_mask, use_mask=use_mask, binarize_mask=binarize_mask, cache_masked_activations=cache_masked_activations, cache_sae_activations=cache_sae_activations, mean_mask=mean_mask, mean_ablate=mean_ablate, fake_activations=fake_activations, ig_mask_threshold=ig_mask_threshold),\n",
    "            )\n",
    "        )\n",
    "    return hooks \n",
    "\n",
    "def build_sae_logitfn(**kwargs):\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, **kwargs)\n",
    "            )\n",
    "    return logitfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clean_prefix': 'Then, Richard and Erica went to the hospital. Richard gave a drink to', 'patch_prefix': 'Then, Matthew and Crystal went to the hospital. Megan gave a drink to', 'clean_answer': ' Erica', 'patch_answer': ' Richard', 'case': 'BABA_16'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = 'data/ioi/ioi_train.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "for entry in data:\n",
    "    print(entry)\n",
    "    break\n",
    "example_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/ioi/ioi_train_21.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Then', ',', ' Matthew', ' and', ' Crystal', ' went', ' to', ' the', ' hospital', '.', ' Megan', ' gave', ' a', ' drink', ' to']\n",
      "Tokenized answer: [' Erica']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">667</span><span style=\"font-weight: bold\">      Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">17.49</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: | Erica|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m667\u001b[0m\u001b[1m      Logit: \u001b[0m\u001b[1;36m17.49\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: | Erica|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 28.58 Prob: 43.22% Token: | Matthew|\n",
      "Top 1th token. Logit: 27.45 Prob: 13.99% Token: | Crystal|\n",
      "Top 2th token. Logit: 27.27 Prob: 11.63% Token: | her|\n",
      "Top 3th token. Logit: 27.19 Prob: 10.73% Token: | the|\n",
      "Top 4th token. Logit: 26.28 Prob:  4.33% Token: | them|\n",
      "Top 5th token. Logit: 25.45 Prob:  1.88% Token: | both|\n",
      "Top 6th token. Logit: 25.22 Prob:  1.50% Token: | him|\n",
      "Top 7th token. Logit: 25.16 Prob:  1.41% Token: | a|\n",
      "Top 8th token. Logit: 25.14 Prob:  1.39% Token: | each|\n",
      "Top 9th token. Logit: 24.26 Prob:  0.57% Token: | Matt|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Erica'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">667</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Erica'\u001b[0m, \u001b[1;36m667\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "test_prompt(\"Then, Matthew and Crystal went to the hospital. Megan gave a drink to\", \" Erica\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2966\n"
     ]
    }
   ],
   "source": [
    "clean_data = []\n",
    "corr_data = []\n",
    "clean_labels = []\n",
    "corr_labels = []\n",
    "for entry in data:\n",
    "    if model.to_tokens(entry['clean_prefix']).shape[-1] == example_length:\n",
    "        clean_data.append(entry['clean_prefix'])\n",
    "        corr_data.append(entry['patch_prefix'])\n",
    "        clean_labels.append(entry['clean_answer'])\n",
    "        corr_labels.append(entry['patch_answer'])\n",
    "print(len(clean_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2900, 20]) torch.Size([2900, 20])\n",
      "torch.Size([181, 16, 20]) torch.Size([181, 16, 20]) torch.Size([181, 16]) torch.Size([181, 16])\n"
     ]
    }
   ],
   "source": [
    "clean_data = []\n",
    "corr_data = []\n",
    "clean_labels = []\n",
    "corr_labels = []\n",
    "for entry in data:\n",
    "    if model.to_tokens(entry['clean_prefix']).shape[-1] == example_length:\n",
    "        clean_data.append(entry['clean_prefix'])\n",
    "        corr_data.append(entry['patch_prefix'])\n",
    "        clean_labels.append(entry['clean_answer'])\n",
    "        corr_labels.append(entry['patch_answer'])\n",
    "\n",
    "N = 2900\n",
    "clean_tokens = model.to_tokens(clean_data[:N])\n",
    "corr_tokens = model.to_tokens(corr_data[:N])\n",
    "clean_label_tokens = model.to_tokens(clean_labels[:N], prepend_bos=False).squeeze(-1)\n",
    "corr_label_tokens = model.to_tokens(corr_labels[:N], prepend_bos=False).squeeze(-1)\n",
    "print(clean_tokens.shape, corr_tokens.shape)\n",
    "\n",
    "def logit_diff_fn(logits, clean_labels, corr_labels, token_wise=False):\n",
    "    clean_logits = logits[torch.arange(logits.shape[0]), -1, clean_labels]\n",
    "    corr_logits = logits[torch.arange(logits.shape[0]), -1, corr_labels]\n",
    "    return (clean_logits - corr_logits).mean() if not token_wise else (clean_logits - corr_logits)\n",
    "\n",
    "batch_size = 16 \n",
    "clean_tokens = clean_tokens[:batch_size*(len(clean_tokens)//batch_size)]\n",
    "corr_tokens = corr_tokens[:batch_size*(len(corr_tokens)//batch_size)]\n",
    "clean_label_tokens = clean_label_tokens[:batch_size*(len(clean_label_tokens)//batch_size)]\n",
    "corr_label_tokens = corr_label_tokens[:batch_size*(len(corr_label_tokens)//batch_size)]\n",
    "\n",
    "clean_tokens = clean_tokens.reshape(-1, batch_size, clean_tokens.shape[-1])\n",
    "corr_tokens = corr_tokens.reshape(-1, batch_size, corr_tokens.shape[-1])\n",
    "clean_label_tokens = clean_label_tokens.reshape(-1, batch_size)\n",
    "corr_label_tokens = corr_label_tokens.reshape(-1, batch_size)\n",
    "\n",
    "print(clean_tokens.shape, corr_tokens.shape, clean_label_tokens.shape, corr_label_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.3811, device='cuda:0')\n",
      "tensor(4.2698, device='cuda:0')\n",
      "tensor(4.0039, device='cuda:0')\n",
      "tensor(3.3653, device='cuda:0')\n",
      "tensor(3.4589, device='cuda:0')\n",
      "tensor(3.2856, device='cuda:0')\n",
      "tensor(3.1497, device='cuda:0')\n",
      "tensor(3.6056, device='cuda:0')\n",
      "tensor(3.6865, device='cuda:0')\n",
      "tensor(3.4956, device='cuda:0')\n",
      "Average LD:  3.6701977252960205\n"
     ]
    }
   ],
   "source": [
    "use_mask = False \n",
    "mean_mask = False\n",
    "avg_logit_diff = 0\n",
    "cleanup_cuda()\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        logits = model.run_with_hooks(\n",
    "            clean_tokens[i], \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(clean_tokens[i], use_mask=use_mask, mean_mask=mean_mask)\n",
    "            )\n",
    "        ld = logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i])\n",
    "        print(ld)\n",
    "        avg_logit_diff += ld\n",
    "        del logits\n",
    "        cleanup_cuda()\n",
    "avg_logit_diff = (avg_logit_diff / 10).item()\n",
    "print(\"Average LD: \", avg_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sae in saes:\n",
    "    sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Accum Progress: 100%|██████████| 400/400 [01:29<00:00,  4.46it/s]\n"
     ]
    }
   ],
   "source": [
    "def running_mean_tensor(old_mean, new_value, n):\n",
    "    return old_mean + (new_value - old_mean) / n\n",
    "\n",
    "def get_sae_means(mean_tokens, total_batches, batch_size, per_token_mask=False):\n",
    "    for sae in saes:\n",
    "        sae.mean_ablation = torch.zeros(sae.cfg.d_sae).float().to(device)\n",
    "    \n",
    "    with tqdm(total=total_batches*batch_size, desc=\"Mean Accum Progress\") as pbar:\n",
    "        for i in range(total_batches):\n",
    "            for j in range(batch_size):\n",
    "                with torch.no_grad():\n",
    "                    _ = model.run_with_hooks(\n",
    "                        mean_tokens[i, j], \n",
    "                        return_type=\"logits\", \n",
    "                        fwd_hooks=build_hooks_list(mean_tokens[i, j], cache_sae_activations=True)\n",
    "                        )\n",
    "                    for sae in saes:\n",
    "                        sae.mean_ablation = running_mean_tensor(sae.mean_ablation, sae.feature_acts, i+1)\n",
    "                    cleanup_cuda()\n",
    "                pbar.update(1)\n",
    "\n",
    "            if i >= total_batches:\n",
    "                break\n",
    "\n",
    "get_sae_means(corr_tokens, 25, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 16384])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saes[0].mean_ablation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1156, device='cuda:0')\n",
      "tensor(-0.1215, device='cuda:0')\n",
      "tensor(-0.1009, device='cuda:0')\n",
      "tensor(-0.1535, device='cuda:0')\n",
      "tensor(-0.0856, device='cuda:0')\n",
      "Average LD:  tensor(-0.3460, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "avg_mean_diff = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        logits = model.run_with_hooks(\n",
    "            clean_tokens[i], \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(clean_tokens[i], mean_ablate=True)\n",
    "            )\n",
    "        ld = logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i])\n",
    "        print(ld)\n",
    "        avg_mean_diff += ld\n",
    "        del logits\n",
    "        cleanup_cuda()\n",
    "print(\"Average LD: \", avg_mean_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mask training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "class KeyboardInterruptBlocker:\n",
    "    def __enter__(self):\n",
    "        # Block SIGINT and store old mask\n",
    "        self.old_mask = signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGINT})\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore old mask (unblock SIGINT)\n",
    "        signal.pthread_sigmask(signal.SIG_SETMASK, self.old_mask)\n",
    "\n",
    "class Range:\n",
    "    def __init__(self, *args):\n",
    "        # Support for range(start, stop, step) or range(stop)\n",
    "        self.args = args\n",
    "\n",
    "        # Validate input like the built-in range does\n",
    "        if len(self.args) not in {1, 2, 3}:\n",
    "            raise TypeError(f\"Range expected at most 3 arguments, got {len(self.args)}\")\n",
    "        \n",
    "        self.range = __builtins__.range(*self.args)  # Create the range object\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in self.range:\n",
    "            try:\n",
    "                with KeyboardInterruptBlocker():\n",
    "                    yield i\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Keyboard interrupt received. Exiting iteration.\")\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training_run(token_dataset, labels_dataset, corr_labels_dataset, sparsity_multiplier, task, example_length=6, loss_function='ce', per_token_mask=False, use_mask=True, mean_mask=False, distinct_sparsity_multiplier=0 ):\n",
    "\n",
    "    def logitfn(tokens):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask)\n",
    "            )\n",
    "        return logits\n",
    "\n",
    "    def forward_pass(batch, clean_label_tokens, corr_label_tokens, logitfn, ratio_trained=1, loss_function='ce'):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        tokens = batch\n",
    "        logits = logitfn(tokens)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        if loss_function == 'ce':\n",
    "            loss = F.cross_entropy(last_token_logits, clean_label_tokens)\n",
    "        elif loss_function == 'logit_diff':\n",
    "            fwd_logit_diff = logit_diff_fn(logits, clean_label_tokens, corr_label_tokens)\n",
    "            loss = torch.abs(avg_logit_diff - fwd_logit_diff)\n",
    "\n",
    "        sparsity_loss = 0\n",
    "        distinct_sparsity_loss = 0\n",
    "        # if per_token_mask:\n",
    "        #     distinct_sparsity_loss = 0\n",
    "        for sae in saes:\n",
    "            sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "            # if per_token_mask:\n",
    "            #     distinct_sparsity_loss = distinct_sparsity_loss + sae.mask.distinct_sparsity_loss\n",
    "        \n",
    "        sparsity_loss = sparsity_loss / len(saes)\n",
    "        distinct_sparsity_loss = distinct_sparsity_loss / len(saes)\n",
    "\n",
    "        return loss, sparsity_loss, distinct_sparsity_loss\n",
    "\n",
    "    print(\"doing a run with sparsity multiplier\", sparsity_multiplier)\n",
    "    all_optimized_params = []\n",
    "    config = {\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 1,\n",
    "        \"total_steps\": token_dataset.shape[0]*0.8,\n",
    "        \"sparsity_multiplier\": sparsity_multiplier\n",
    "    }\n",
    "\n",
    "    for sae in saes:\n",
    "        if per_token_mask:\n",
    "            sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length).to(device)\n",
    "        else:\n",
    "            sae.mask = SparseMask(sae.cfg.d_sae, 1.0).to(device)\n",
    "        all_optimized_params.extend(list(sae.mask.parameters()))\n",
    "        sae.mask.max_temp = torch.tensor(200.0)\n",
    "    \n",
    "    wandb.init(project=\"sae circuits\", config=config)\n",
    "    optimizer = optim.Adam(all_optimized_params, lr=config[\"learning_rate\"])\n",
    "    total_steps = config[\"total_steps\"] #*20 #*config[\"batch_size\"]\n",
    "    # epochs = 20\n",
    "    with tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n",
    "        # for epoch in range(epochs):\n",
    "        for i, (x, y, z) in enumerate(zip(token_dataset, labels_dataset, corr_labels_dataset)):\n",
    "            with KeyboardInterruptBlocker():\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Calculate ratio trained\n",
    "                ratio_trained =  i / total_steps*1.1\n",
    "                \n",
    "                # Update mask ratio for each SAE\n",
    "                for sae in saes:\n",
    "                    sae.mask.ratio_trained = ratio_trained\n",
    "                \n",
    "                # Forward pass with updated ratio_trained\n",
    "                loss, sparsity_loss, _ = forward_pass(x, y, z, logitfn, ratio_trained=ratio_trained, loss_function=loss_function)\n",
    "                # if per_token_mask:\n",
    "                #     sparsity_loss = sparsity_loss / example_length\n",
    "                # print(\"sp loss\", sparsity_loss)\n",
    "                # print(\"sae 0 sp loss\", saes[0].mask.sparsity_loss)\n",
    "                # print(\"sae 1 sp loss\", saes[1].mask.sparsity_loss)\n",
    "                # print(\"sae 2 sp loss\", saes[2].mask.sparsity_loss)\n",
    "                # print(\"sae 3 sp loss\", saes[3].mask.sparsity_loss)\n",
    "                \n",
    "                avg_nonzero_elements = sparsity_loss\n",
    "                # avg_distinct_nonzero_elements = distinct_sparsity_loss\n",
    "                    \n",
    "                sparsity_loss = sparsity_loss * config[\"sparsity_multiplier\"] #+ distinct_sparsity_loss * distinct_sparsity_multiplier\n",
    "                total_loss = sparsity_loss + loss\n",
    "                infodict  = {\"Step\": i, \"Progress\": ratio_trained, \"Avg Nonzero Elements\": avg_nonzero_elements.item(), \"Task Loss\": loss.item(), \"Sparsity Loss\": sparsity_loss.item(), \"temperature\": saes[0].mask.temperature} # \"avg distinct lat/sae\":avg_distinct_nonzero_elements.item()\n",
    "                wandb.log(infodict)\n",
    "                # break\n",
    "                # Backward pass and optimizer step\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update tqdm bar with relevant metrics\n",
    "                pbar.set_postfix(infodict)\n",
    "                \n",
    "                # Update the tqdm progress bar\n",
    "                pbar.update(1)\n",
    "                # break\n",
    "                if i >= total_steps*1.1:\n",
    "                    break\n",
    "    wandb.finish()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for sae in saes:\n",
    "        for param in sae.parameters():\n",
    "            param.grad = None\n",
    "        for param in sae.mask.parameters():\n",
    "            param.grad = None\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    mask_dict = {}\n",
    "\n",
    "    total_density = 0\n",
    "    for sae in saes:\n",
    "        mask_dict[sae.cfg.hook_name] = torch.where(sae.mask.mask > 0)[1].tolist()   # rob thinks .view(-1) needed here\n",
    "        total_density += (sae.mask.mask > 0).sum().item()\n",
    "    mask_dict[\"total_density\"] = total_density\n",
    "    mask_dict['avg_density'] = total_density / len(saes)\n",
    "\n",
    "    if per_token_mask:\n",
    "        print(\"total # latents in circuit: \", total_density)\n",
    "    print(\"avg density\", mask_dict['avg_density'])\n",
    "\n",
    "    ### EVAL ###\n",
    "    def masked_logit_fn(tokens):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask, binarize_mask=True)\n",
    "            )\n",
    "        return logits\n",
    "\n",
    "    def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        tokens = batch\n",
    "        logits = logitfn(tokens)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        loss = F.cross_entropy(last_token_logits, labels)\n",
    "        return loss\n",
    "    \n",
    "    def eval_logit_diff(num_batches, batch, clean_labels, corr_labels, logitfn, ratio_trained=10):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        avg_ld = 0\n",
    "        for i in range(num_batches):\n",
    "            tokens = batch[-i]\n",
    "            logits = logitfn(tokens)\n",
    "            ld = logit_diff_fn(logits, clean_labels[-i], corr_labels[-i])\n",
    "            avg_ld += ld\n",
    "            del logits\n",
    "            cleanup_cuda()\n",
    "        return (avg_ld / num_batches).item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = eval_ce_loss(token_dataset[-1], labels_dataset[-1], masked_logit_fn)\n",
    "        print(\"CE loss:\", loss)\n",
    "        cleanup_cuda()\n",
    "        logit_diff = eval_logit_diff(10, token_dataset, labels_dataset, corr_labels_dataset, masked_logit_fn)\n",
    "        print(\"Logit Diff:\", logit_diff)\n",
    "        cleanup_cuda()\n",
    "\n",
    "    save_path = f\"masks/{task}/{loss_function}_{str(sparsity_multiplier)}_run/\"\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    mask_dict['ce_loss'] = loss.item()\n",
    "    mask_dict['logit_diff'] = logit_diff\n",
    "    faithfulness = logit_diff / avg_logit_diff\n",
    "    mask_dict['faithfulness'] = faithfulness\n",
    "    \n",
    "    for idx, sae in enumerate(saes):\n",
    "        mask_path = f\"sae_mask_{idx}.pt\"\n",
    "        torch.save(sae.mask.state_dict(), os.path.join(save_path,mask_path))\n",
    "        print(f\"Saved mask for SAE {idx} to {mask_path}\")\n",
    "\n",
    "    json.dump(mask_dict, open(os.path.join(save_path,f\"{str(sparsity_multiplier)}_run.json\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing a run with sparsity multiplier 100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ywv5gfbq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082bdb76df6b4ca5aa9430a13f1ebdbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg Nonzero Elements</td><td>▁▁▂▂▃▄▄▅▅▆▆▇▇▇██████████████████████████</td></tr><tr><td>Progress</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Sparsity Loss</td><td>▁▁▂▂▃▄▄▅▅▆▆▇▇▇██████████████████████████</td></tr><tr><td>Step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Task Loss</td><td>█▇▇█▅▃▆▄▃▄▃▃▂▃▁▂▃▂▂▂▂▄▂▃▁▃▁▃▁▂▂▁▂▁▁▁▁▃▂▂</td></tr><tr><td>temperature</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg Nonzero Elements</td><td>16384.0</td></tr><tr><td>Progress</td><td>0.75967</td></tr><tr><td>Sparsity Loss</td><td>1638400.0</td></tr><tr><td>Step</td><td>100</td></tr><tr><td>Task Loss</td><td>0.67491</td></tr><tr><td>temperature</td><td>55.97834</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">revived-puddle-76</strong> at: <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/ywv5gfbq' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/ywv5gfbq</a><br/> View project at: <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241230_040517-ywv5gfbq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ywv5gfbq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a6bb0d32f84456a7d1eeca5d503222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111281055620768, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/wandb/run-20241230_040643-2ch80mko</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/2ch80mko' target=\"_blank\">ethereal-dawn-77</a></strong> to <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/2ch80mko' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/2ch80mko</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  99%|█████████▉| 144/144.8 [01:53<00:00,  1.28it/s, Step=144, Progress=1.09, Avg Nonzero Elements=16384.0, Task Loss=0.774, Sparsity Loss=1.64e+6, temperature=tensor(328.9644)]  /home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/tqdm/std.py:636: TqdmWarning: clamping frac to range [0, 1]\n",
      "  full_bar = Bar(frac,\n",
      "Training Progress: 161it [02:06,  1.28it/s, Step=160, Progress=1.22, Avg Nonzero Elements=16384.0, Task Loss=0.517, Sparsity Loss=1.64e+6, temperature=tensor(626.3716)]                           \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2df4c0f55b344e3b10a586d3768987b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.021 MB of 0.021 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg Nonzero Elements</td><td>▁▂▃▄▅▆▆▇▇███████████████████████████████</td></tr><tr><td>Progress</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Sparsity Loss</td><td>▁▂▃▄▅▆▆▇▇███████████████████████████████</td></tr><tr><td>Step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>Task Loss</td><td>█▇█▄▄▃▃▂▃▁▃▂▁▃▁▁▂▃▂▂▄▂▁▃▂▂▂▂▂▂▁▃▂▁▆▂▂▂▂▁</td></tr><tr><td>temperature</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▅▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg Nonzero Elements</td><td>16384.0</td></tr><tr><td>Progress</td><td>1.21547</td></tr><tr><td>Sparsity Loss</td><td>1638400.0</td></tr><tr><td>Step</td><td>160</td></tr><tr><td>Task Loss</td><td>0.5165</td></tr><tr><td>temperature</td><td>626.37158</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ethereal-dawn-77</strong> at: <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/2ch80mko' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/2ch80mko</a><br/> View project at: <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241230_040643-2ch80mko/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/ioi_run.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgpu024.unity.rc.umass.edu/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/ioi_run.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m do_training_run(token_dataset \u001b[39m=\u001b[39;49mclean_tokens, labels_dataset\u001b[39m=\u001b[39;49m clean_label_tokens, corr_labels_dataset\u001b[39m=\u001b[39;49mcorr_label_tokens, sparsity_multiplier\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, task\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mioi/baba21/\u001b[39;49m\u001b[39m'\u001b[39;49m, example_length\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, loss_function\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlogit_diff\u001b[39;49m\u001b[39m\"\u001b[39;49m, per_token_mask\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, use_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, mean_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/ioi_run.ipynb Cell 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu024.unity.rc.umass.edu/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/ioi_run.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m total_density \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu024.unity.rc.umass.edu/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/ioi_run.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39mfor\u001b[39;00m sae \u001b[39min\u001b[39;00m saes:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgpu024.unity.rc.umass.edu/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/ioi_run.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m     mask_dict[sae\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mhook_name] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(sae\u001b[39m.\u001b[39;49mmask\u001b[39m.\u001b[39;49mmask \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m)[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mtolist()   \u001b[39m# rob thinks .view(-1) needed here\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu024.unity.rc.umass.edu/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/ioi_run.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m     total_density \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (sae\u001b[39m.\u001b[39mmask\u001b[39m.\u001b[39mmask \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgpu024.unity.rc.umass.edu/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/ioi_run.ipynb#X32sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m mask_dict[\u001b[39m\"\u001b[39m\u001b[39mtotal_density\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m total_density\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "do_training_run(token_dataset =clean_tokens, labels_dataset= clean_label_tokens, corr_labels_dataset=corr_label_tokens, sparsity_multiplier=100, task='ioi/baba21/', example_length=20, loss_function=\"logit_diff\", per_token_mask=False, use_mask=True, mean_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(262144., device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(saes[0].mask.mask).sum() * saes[0].mask.distinct_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16384., device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(saes[0].mask.mask).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saes[0].mask.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7311, 0.7311, 0.7311,  ..., 0.7311, 0.7311, 0.7311], device='cuda:0',\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(saes[0].mask.mask * saes[0].mask.temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(191642.6094, device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saes[0].mask.sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(262144, device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(saes[0].mask.mask == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(262144, device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(saes[0].mask.mask>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16056*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.04"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8*1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jnainani_umass_edu/.conda/envs/finetuning/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/data/ioi/ioi_dataset.py:634: UserWarning: S2 index has been computed as the same for S and S2\n",
      "  warnings.warn(\"S2 index has been computed as the same for S and S2\")\n"
     ]
    }
   ],
   "source": [
    "from data.ioi import ioi_dataset\n",
    "\n",
    "ioi_data = ioi_dataset.IOIDataset(prompt_type=\"BABA\",nb_templates=2, N = 6000)\n",
    "abc_data = (\n",
    "    ioi_data.gen_flipped_prompts((\"IO\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S\", \"RAND\"))\n",
    "    .gen_flipped_prompts((\"S1\", \"RAND\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2966\n"
     ]
    }
   ],
   "source": [
    "lower = 0\n",
    "i = 0\n",
    "for pr in ioi_data.ioi_prompts:\n",
    "    ll = len(model.tokenizer.encode(pr['text']))\n",
    "    # print(ll)\n",
    "    # print(pr['text'])\n",
    "    if ll == 21:\n",
    "        lower += 1\n",
    "    # i += 1\n",
    "    # if i > 10:\n",
    "    #     break\n",
    "print(lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def truncate_to_end_with_to(text):\n",
    "    return re.sub(r'\\s+\\S+$', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2966"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_ioi_dataset = []\n",
    "for ind, pr in enumerate(ioi_data.ioi_prompts):\n",
    "    pr_abc = abc_data.ioi_prompts[ind]\n",
    "    ll = len(model.tokenizer.encode(pr['text']))\n",
    "    labv = len(model.tokenizer.encode(pr_abc['text']))\n",
    "    if ll == 21 and ll == labv:\n",
    "        ioi_dict = {\"clean_prefix\": truncate_to_end_with_to(pr['text']), \"patch_prefix\": truncate_to_end_with_to(pr_abc['text']), \"clean_answer\": ' '+pr['IO'], \"patch_answer\": ' '+pr['S'], \"case\": \"BABA_21\"}\n",
    "        full_ioi_dataset.append(ioi_dict)\n",
    "\n",
    "len(full_ioi_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the ioi dataset\n",
    "with open('data/ioi_train_21.json', 'w') as outfile:\n",
    "    json.dump(full_ioi_dataset, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "file_path = 'data/ioi/ioi_train_21.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

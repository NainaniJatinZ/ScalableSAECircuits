{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In IPython\n",
      "Set autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "import helpers.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_fn(logits, clean_labels, corr_labels, token_wise=False):\n",
    "    \"\"\"\n",
    "    Compute logit difference for a batch.\n",
    "    \"\"\"\n",
    "    # logits shape: [batch, seq_len, vocab_size]\n",
    "    clean_logits = logits[torch.arange(logits.shape[0]), -1, clean_labels]\n",
    "    corr_logits = logits[torch.arange(logits.shape[0]), -1, corr_labels]\n",
    "    if token_wise:\n",
    "        return (clean_logits - corr_logits)\n",
    "    else:\n",
    "        return (clean_logits - corr_logits).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f747d81730af4b549f7d9e9012682a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "# model_name = \"google/gemma-2-9b\"\n",
    "model = HookedSAETransformer.from_pretrained(model_name, device=device, cache_dir=hf_cache) \n",
    "\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_l0_target = 100\n",
    "sae_gap = 5\n",
    "\n",
    "df = pd.DataFrame.from_records({k:v.__dict__ for k,v in get_pretrained_saes_directory().items()}).T\n",
    "df.drop(columns=[\"expected_var_explained\", \"expected_l0\", \"config_overrides\", \"conversion_func\"], inplace=True)\n",
    "# df.loc['gemma-scope-2b-pt-res']['saes_map'] #['neuronpedia_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cfg.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'layer_0/width_16k/average_l0_105', 1: 'layer_1/width_16k/average_l0_102', 2: 'layer_2/width_16k/average_l0_141', 3: 'layer_3/width_16k/average_l0_59', 4: 'layer_4/width_16k/average_l0_124', 5: 'layer_5/width_16k/average_l0_68', 6: 'layer_6/width_16k/average_l0_70', 7: 'layer_7/width_16k/average_l0_69', 8: 'layer_8/width_16k/average_l0_71', 9: 'layer_9/width_16k/average_l0_73', 10: 'layer_10/width_16k/average_l0_77', 11: 'layer_11/width_16k/average_l0_80', 12: 'layer_12/width_16k/average_l0_82', 13: 'layer_13/width_16k/average_l0_84', 14: 'layer_14/width_16k/average_l0_84', 15: 'layer_15/width_16k/average_l0_78', 16: 'layer_16/width_16k/average_l0_78', 17: 'layer_17/width_16k/average_l0_77', 18: 'layer_18/width_16k/average_l0_74', 19: 'layer_19/width_16k/average_l0_73', 20: 'layer_20/width_16k/average_l0_71', 21: 'layer_21/width_16k/average_l0_70', 22: 'layer_22/width_16k/average_l0_72', 23: 'layer_23/width_16k/average_l0_75', 24: 'layer_24/width_16k/average_l0_73', 25: 'layer_25/width_16k/average_l0_116'}\n",
      "[0, 3, 6, 9, 12, 15, 18, 21, 24]\n"
     ]
    }
   ],
   "source": [
    "sae_gap = 3\n",
    "# # This is your custom approach from the notebook:\n",
    "neuronpedia_dict = df.loc['gemma-scope-2b-pt-res']['saes_map']\n",
    "pattern = re.compile(r'layer_(\\d+)/width_16k/average_l0_(\\d+)')\n",
    "layer_dict = defaultdict(list)\n",
    "for s in neuronpedia_dict.keys():\n",
    "    match = pattern.search(s)\n",
    "    if match and neuronpedia_dict[s] is not None:\n",
    "        layer = int(match.group(1))\n",
    "        l0_value = int(match.group(2))\n",
    "        layer_dict[layer].append((s, l0_value))\n",
    "\n",
    "# Find the string with l0 value closest to the user-specified target\n",
    "closest_strings = {}\n",
    "for layer, items in layer_dict.items():\n",
    "    closest_string = min(items, key=lambda x: abs(x[1] - layer_l0_target))\n",
    "    closest_strings[layer] = closest_string[0]\n",
    "print(closest_strings)\n",
    "# Actually load the SAEs\n",
    "layers = [i for i in range(0, model.cfg.n_layers, sae_gap)]\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:00<00:04,  1.61it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376fe7fee7db4178a1e4ad1ab85dcc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:09<00:37,  5.29s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a3ee0487a64dccaa6cb55caf85447b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:17<00:40,  6.79s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f7c7462c834d6abd08416bdf36280c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:26<00:37,  7.48s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e64e35227df4a9490656eee961a26ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:35<00:16,  5.39s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5881482ec84233ae1f6bf412e4cacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:43<00:12,  6.42s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12f1cccb7fb402c91a5cb9bb4b75aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:52<00:07,  7.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f4f8c596b9483db223a1c7685c9fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [01:01<00:00,  6.80s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "saes = []\n",
    "for layer in tqdm(layers):\n",
    "    sae_id = closest_strings.get(layer, None)\n",
    "    if sae_id is not None:\n",
    "        sae = SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res\",\n",
    "            sae_id=sae_id,\n",
    "            device=device\n",
    "        )[0]\n",
    "        saes.append(sae)\n",
    "    else:\n",
    "        print(f\"Warning: No matching SAE ID found for layer {layer} with L0 target {layer_l0_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total batches after reshape: 187\n"
     ]
    }
   ],
   "source": [
    "task = \"sva/rc_train\"\n",
    "example_length = 7\n",
    "N = 3000\n",
    "batch_size = 16\n",
    "\n",
    "data_path = f\"data/{task}.json\"\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Data file {data_path} not found.\")\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "clean_data = []\n",
    "corr_data = []\n",
    "clean_labels = []\n",
    "corr_labels = []\n",
    "\n",
    "for entry in data:\n",
    "    clean_len = len(model.tokenizer(entry['clean_prefix']).input_ids)\n",
    "    corr_len = len(model.tokenizer(entry['patch_prefix']).input_ids)\n",
    "    if clean_len == corr_len == example_length:\n",
    "        clean_data.append(entry['clean_prefix'])\n",
    "        corr_data.append(entry['patch_prefix'])\n",
    "        clean_labels.append(entry['clean_answer'])\n",
    "        corr_labels.append(entry['patch_answer'])\n",
    "\n",
    "# Limit to top N\n",
    "clean_data = clean_data[:N]\n",
    "corr_data = corr_data[:N]\n",
    "clean_labels = clean_labels[:N]\n",
    "corr_labels = corr_labels[:N]\n",
    "\n",
    "# Tokenize\n",
    "clean_tokens = model.to_tokens(clean_data)\n",
    "corr_tokens = model.to_tokens(corr_data)\n",
    "clean_label_tokens = model.to_tokens(clean_labels, prepend_bos=False).squeeze(-1)\n",
    "corr_label_tokens = model.to_tokens(corr_labels, prepend_bos=False).squeeze(-1)\n",
    "\n",
    "# Reshape into batches\n",
    "n_batches_total = (len(clean_tokens) // batch_size)\n",
    "clean_tokens = clean_tokens[:n_batches_total*batch_size]\n",
    "corr_tokens = corr_tokens[:n_batches_total*batch_size]\n",
    "clean_label_tokens = clean_label_tokens[:n_batches_total*batch_size]\n",
    "corr_label_tokens = corr_label_tokens[:n_batches_total*batch_size]\n",
    "\n",
    "clean_tokens = clean_tokens.reshape(-1, batch_size, clean_tokens.shape[-1])\n",
    "corr_tokens = corr_tokens.reshape(-1, batch_size, corr_tokens.shape[-1])\n",
    "clean_label_tokens = clean_label_tokens.reshape(-1, batch_size)\n",
    "corr_label_tokens = corr_label_tokens.reshape(-1, batch_size)\n",
    "\n",
    "print(\"Number of total batches after reshape:\", clean_tokens.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing average full model logit diff (clean vs corr)...\n",
      "Average Full Model LD: 3.532318115234375\n"
     ]
    }
   ],
   "source": [
    "print(\"Computing average full model logit diff (clean vs corr)...\")\n",
    "avg_model_diff = 0.0\n",
    "num_batches_eval = 10\n",
    "# utils.cleanup_cuda()\n",
    "with torch.no_grad():\n",
    "    for i in range(min(num_batches_eval, clean_tokens.shape[0])):\n",
    "        logits = model(clean_tokens[i])  # shape [batch_size, seq_len, vocab_size]\n",
    "        ld = logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i])\n",
    "        avg_model_diff += ld\n",
    "avg_model_diff = (avg_model_diff / min(num_batches_eval, clean_tokens.shape[0])).item()\n",
    "print(\"Average Full Model LD:\", avg_model_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model + SAEs LD: 1.569484829902649\n"
     ]
    }
   ],
   "source": [
    "avg_logit_diff = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(min(num_batches_eval, clean_tokens.shape[0])):\n",
    "        logits, saes = utils.run_sae_hook_fn(model, saes, clean_tokens[i], use_mean_error=False)\n",
    "        ld = logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i])\n",
    "        avg_logit_diff += ld\n",
    "avg_logit_diff = (avg_logit_diff / min(num_batches_eval, clean_tokens.shape[0])).item()\n",
    "print(\"Average Model + SAEs LD:\", avg_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model + SAEs LD: 3.532318115234375\n"
     ]
    }
   ],
   "source": [
    "avg_logit_diff = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(min(num_batches_eval, clean_tokens.shape[0])):\n",
    "        logits, saes = utils.run_sae_hook_fn(model, saes, clean_tokens[i], use_mean_error=False, calc_error=True, use_error=True)\n",
    "        ld = logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i])\n",
    "        avg_logit_diff += ld\n",
    "avg_logit_diff = (avg_logit_diff / min(num_batches_eval, clean_tokens.shape[0])).item()\n",
    "print(\"Average Model + SAEs LD:\", avg_logit_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Accum Progress:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Accum Progress: 100%|██████████| 640/640 [02:39<00:00,  4.02it/s]\n",
      "Mean Accum Progress: 100%|██████████| 640/640 [02:38<00:00,  4.04it/s]\n"
     ]
    }
   ],
   "source": [
    "for sae in saes:\n",
    "    sae.mask = utils.SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length).to(device)\n",
    "saes = utils.get_sae_means(model, saes, corr_tokens, total_batches=40, batch_size=16)\n",
    "saes = utils.get_sae_error_means(model, saes, corr_tokens, total_batches=40, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Model + SAEs (Mean Error) LD: 2.1713619232177734\n"
     ]
    }
   ],
   "source": [
    "avg_logit_err_diff = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(min(num_batches_eval, clean_tokens.shape[0])):\n",
    "        logits, saes = utils.run_sae_hook_fn(\n",
    "            model, saes, clean_tokens[i], use_mean_error=True\n",
    "        )\n",
    "        ld = logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i])\n",
    "        avg_logit_err_diff += ld\n",
    "avg_logit_err_diff = (avg_logit_err_diff / min(num_batches_eval, clean_tokens.shape[0])).item()\n",
    "print(\"Average Model + SAEs (Mean Error) LD:\", avg_logit_err_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

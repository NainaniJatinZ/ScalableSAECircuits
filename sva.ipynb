{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from functools import partial\n",
    "import einops\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ") \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from functools import lru_cache\n",
    "from typing import TypedDict, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700d9a30746a4e1bbd84c10cf109b920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-9b\", device=device, cache_dir=hf_cache) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = model.tokenizer.pad_token_id\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "device = \"cuda\"\n",
    "layers= [7, 14, 21, 40]\n",
    "l0s = [92, 67, 129, 125]\n",
    "saes = [SAE.from_pretrained(release=\"gemma-scope-9b-pt-res\", sae_id=f\"layer_{layers[i]}/width_16k/average_l0_{l0s[i]}\", device=device)[0] for i in range(len(layers))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cuda():\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "\n",
    "def clear_memory():\n",
    "   for sae in saes:\n",
    "      for param in sae.parameters():\n",
    "         param.grad = None\n",
    "      for param in sae.mask.parameters():\n",
    "         param.grad = None\n",
    "\n",
    "   for param in model.parameters():\n",
    "      param.grad = None\n",
    "   cleanup_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMask(nn.Module):\n",
    "    def __init__(self, shape, l1, seq_len=None, distinct_l1=0):\n",
    "        super().__init__()\n",
    "        if seq_len is not None:\n",
    "            self.mask = nn.Parameter(torch.ones(seq_len, shape))\n",
    "        else:\n",
    "            self.mask = nn.Parameter(torch.ones(shape))\n",
    "        self.l1 = l1\n",
    "        self.distinct_l1 = distinct_l1\n",
    "        self.max_temp = torch.tensor(1000.0)\n",
    "        self.sparsity_loss = None\n",
    "        self.ratio_trained = 1\n",
    "        self.temperature = 1\n",
    "        self.distinct_sparsity_loss = 0\n",
    "\n",
    "\n",
    "    def forward(self, x, binary=False, mean_ablation=None):\n",
    "        if binary:\n",
    "            # binary mask, 0 if negative, 1 if positive\n",
    "            binarized = (self.mask > 0).float()\n",
    "            if mean_ablation is None:\n",
    "                return x * binarized\n",
    "            else:\n",
    "                diff = x - mean_ablation\n",
    "                return diff * binarized + mean_ablation\n",
    "            \n",
    "\n",
    "        self.temperature = self.max_temp ** self.ratio_trained\n",
    "        mask = torch.sigmoid(self.mask * self.temperature)\n",
    "        self.sparsity_loss = torch.abs(mask).sum() * self.distinct_l1\n",
    "        if len(mask.shape) == 2:\n",
    "            self.distinct_sparsity_loss = torch.abs(mask).max(dim=0).values.sum() * self.l1\n",
    "\n",
    "        if mean_ablation is None:\n",
    "            return x * mask\n",
    "        else:\n",
    "            diff = x - mean_ablation\n",
    "            return diff * mask + mean_ablation\n",
    "\n",
    "# for sae in saes:\n",
    "#     sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = model.tokenizer.bos_token_id\n",
    "\n",
    "def build_sae_hook_fn(\n",
    "    sae,\n",
    "    sequence,\n",
    "    circuit_mask=None,\n",
    "    use_mask=False,\n",
    "    binarize_mask=False,\n",
    "    mean_mask=False,\n",
    "    ig_mask_threshold=None,\n",
    "    cache_sae_grads=False,\n",
    "    cache_masked_activations=False,\n",
    "    cache_sae_activations=False,\n",
    "    mean_ablate=False, \n",
    "    fake_activations=False):    \n",
    "    \n",
    "    mask = torch.ones_like(sequence, dtype=torch.bool)\n",
    "    mask[sequence == bos_token_id] = False \n",
    "\n",
    "    def sae_hook(value, hook):\n",
    "        # print(f\"sae {sae.cfg.hook_name} running at layer {hook.layer()}\")\n",
    "        feature_acts = sae.encode(value)\n",
    "        feature_acts = feature_acts * mask.unsqueeze(-1)\n",
    "        if fake_activations != False and sae.cfg.hook_layer == fake_activations[0]:\n",
    "            feature_acts = fake_activations[1]\n",
    "        if cache_sae_grads:\n",
    "            raise NotImplementedError(\"torch is confusing\")\n",
    "            sae.feature_acts = feature_acts.requires_grad_(True)\n",
    "            sae.feature_acts.retain_grad()\n",
    "        \n",
    "        if cache_sae_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        \n",
    "        # Learned Binary Masking\n",
    "        if use_mask:\n",
    "            if mean_mask:\n",
    "                # apply the mask, with mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                # apply the mask, without mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask)\n",
    "\n",
    "        # IG Masking\n",
    "        if ig_mask_threshold != None:\n",
    "            # apply the ig mask\n",
    "            if mean_mask:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold)\n",
    "\n",
    "        if circuit_mask is not None:\n",
    "            raise NotImplementedError(\"mask interface not supported\")\n",
    "            mask_method = circuit_mask['mask_method']\n",
    "            mask_indices = circuit_mask[sae.cfg.hook_name]\n",
    "            if mask_method == 'keep_only':\n",
    "                # any activations not in the mask are set to 0\n",
    "                expanded_circuit_mask = torch.zeros_like(feature_acts)\n",
    "                expanded_circuit_mask[:, :, mask_indices] = 1\n",
    "                feature_acts = feature_acts * expanded_circuit_mask\n",
    "            elif mask_method == 'zero_only':\n",
    "                feature_acts[:, :, mask_indices] = 0\n",
    "            else:\n",
    "                raise ValueError(f\"mask_method {mask_method} not recognized\")\n",
    "            \n",
    "        if cache_masked_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        if mean_ablate:\n",
    "            feature_acts = sae.mean_ablation\n",
    "\n",
    "        out = sae.decode(feature_acts)\n",
    "        # choose out or value based on the mask\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(value)\n",
    "        value = torch.where(mask_expanded, out, value)\n",
    "        return value\n",
    "    return sae_hook\n",
    "\n",
    "def build_hooks_list(sequence,\n",
    "                    cache_sae_activations=False,\n",
    "                    cache_sae_grads=False,\n",
    "                    circuit_mask=None,\n",
    "                    use_mask=False,\n",
    "                    binarize_mask=False,\n",
    "                    mean_mask=False,\n",
    "                    cache_masked_activations=False,\n",
    "                    mean_ablate=False,\n",
    "                    fake_activations: Tuple[int, torch.Tensor] = False,\n",
    "                    ig_mask_threshold=None,\n",
    "                    ):\n",
    "    hooks = []\n",
    "    for sae in saes:\n",
    "        hooks.append(\n",
    "            (\n",
    "            sae.cfg.hook_name,\n",
    "            build_sae_hook_fn(sae, sequence, cache_sae_grads=cache_sae_grads, circuit_mask=circuit_mask, use_mask=use_mask, binarize_mask=binarize_mask, cache_masked_activations=cache_masked_activations, cache_sae_activations=cache_sae_activations, mean_mask=mean_mask, mean_ablate=mean_ablate, fake_activations=fake_activations, ig_mask_threshold=ig_mask_threshold),\n",
    "            )\n",
    "        )\n",
    "    return hooks \n",
    "\n",
    "def build_sae_logitfn(**kwargs):\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, **kwargs)\n",
    "            )\n",
    "    return logitfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clean_prefix': 'The friends that the dancer visits', 'patch_prefix': 'The friend that the dancer visits', 'clean_answer': ' go', 'patch_answer': ' goes', 'case': 'plural_singular'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = 'data/sva/rc_train.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "for entry in data:\n",
    "    print(entry)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'The', ' athlete', ' that', ' the', ' chefs', ' hate']\n",
      "Tokenized answer: [' are']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.10</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.09</span><span style=\"font-weight: bold\">% Token: | are|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m49\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m24.10\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.09\u001b[0m\u001b[1m% Token: | are|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 29.52 Prob: 20.86% Token: | the|\n",
      "Top 1th token. Logit: 29.34 Prob: 17.35% Token: | to|\n",
      "Top 2th token. Logit: 28.84 Prob: 10.49% Token: |.|\n",
      "Top 3th token. Logit: 28.45 Prob:  7.13% Token: |,|\n",
      "Top 4th token. Logit: 28.29 Prob:  6.06% Token: | is|\n",
      "Top 5th token. Logit: 28.13 Prob:  5.18% Token: |\n",
      "\n",
      "|\n",
      "Top 6th token. Logit: 28.01 Prob:  4.57% Token: | most|\n",
      "Top 7th token. Logit: 27.45 Prob:  2.63% Token: |?|\n",
      "Top 8th token. Logit: 27.33 Prob:  2.33% Token: | and|\n",
      "Top 9th token. Logit: 27.31 Prob:  2.27% Token: |!|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' are'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' are'\u001b[0m, \u001b[1;36m49\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "test_prompt(\"The athlete that the chefs hate\", \" are\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "corr_data = []\n",
    "clean_labels = []\n",
    "corr_labels = []\n",
    "for entry in data:\n",
    "    if model.to_tokens(entry['clean_prefix']).shape[-1] == 7:\n",
    "        clean_data.append(entry['clean_prefix'])\n",
    "        corr_data.append(entry['patch_prefix'])\n",
    "        clean_labels.append(entry['clean_answer'])\n",
    "        corr_labels.append(entry['patch_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 7]) torch.Size([10000, 7])\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "clean_tokens = model.to_tokens(clean_data[:N])\n",
    "corr_tokens = model.to_tokens(corr_data[:N])\n",
    "clean_label_tokens = model.to_tokens(clean_labels[:N], prepend_bos=False).squeeze(-1)\n",
    "corr_label_tokens = model.to_tokens(corr_labels[:N], prepend_bos=False).squeeze(-1)\n",
    "print(clean_tokens.shape, corr_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diff_fn(logits, clean_labels, corr_labels, token_wise=False):\n",
    "    clean_logits = logits[torch.arange(logits.shape[0]), -1, clean_labels]\n",
    "    corr_logits = logits[torch.arange(logits.shape[0]), -1, corr_labels]\n",
    "    return (clean_logits - corr_logits).mean() if not token_wise else (clean_logits - corr_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([625, 16, 7]) torch.Size([625, 16, 7]) torch.Size([625, 16]) torch.Size([625, 16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16 \n",
    "clean_tokens = clean_tokens[:batch_size*(len(clean_tokens)//batch_size)]\n",
    "corr_tokens = corr_tokens[:batch_size*(len(corr_tokens)//batch_size)]\n",
    "clean_label_tokens = clean_label_tokens[:batch_size*(len(clean_label_tokens)//batch_size)]\n",
    "corr_label_tokens = corr_label_tokens[:batch_size*(len(corr_label_tokens)//batch_size)]\n",
    "\n",
    "clean_tokens = clean_tokens.reshape(-1, batch_size, clean_tokens.shape[-1])\n",
    "corr_tokens = corr_tokens.reshape(-1, batch_size, corr_tokens.shape[-1])\n",
    "clean_label_tokens = clean_label_tokens.reshape(-1, batch_size)\n",
    "corr_label_tokens = corr_label_tokens.reshape(-1, batch_size)\n",
    "\n",
    "print(clean_tokens.shape, corr_tokens.shape, clean_label_tokens.shape, corr_label_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.6336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.7514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.3448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.6474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(4.0851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.2005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.2937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.4509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(3.9535, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "use_mask = False \n",
    "mean_mask = False\n",
    "for i in range(10):\n",
    "    logits = model.run_with_hooks(\n",
    "        clean_tokens[i], \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(clean_tokens[i], use_mask=use_mask, mean_mask=mean_mask)\n",
    "        )\n",
    "    print(logit_diff_fn(logits, clean_label_tokens[i], corr_label_tokens[i]))\n",
    "    del logits\n",
    "    cleanup_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sae in saes:\n",
    "    sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=7).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 7])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Accum Progress:   0%|          | 0/160 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mean Accum Progress: 100%|██████████| 160/160 [00:45<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "def running_mean_tensor(old_mean, new_value, n):\n",
    "    return old_mean + (new_value - old_mean) / n\n",
    "\n",
    "def get_sae_means(mean_tokens, total_batches, batch_size, per_token_mask=False):\n",
    "    for sae in saes:\n",
    "        sae.mean_ablation = torch.zeros(sae.cfg.d_sae).float().to(device)\n",
    "    \n",
    "    with tqdm(total=total_batches*batch_size, desc=\"Mean Accum Progress\") as pbar:\n",
    "        for i in range(total_batches):\n",
    "            for j in range(batch_size):\n",
    "                with torch.no_grad():\n",
    "                    _ = model.run_with_hooks(\n",
    "                        mean_tokens[i, j], \n",
    "                        return_type=\"logits\", \n",
    "                        fwd_hooks=build_hooks_list(clean_tokens[i, j], cache_sae_activations=True)\n",
    "                        )\n",
    "                    for sae in saes:\n",
    "                        sae.mean_ablation = running_mean_tensor(sae.mean_ablation, sae.feature_acts, i+1)\n",
    "                    cleanup_cuda()\n",
    "                pbar.update(1)\n",
    "\n",
    "            if i >= total_batches:\n",
    "                break\n",
    "\n",
    "get_sae_means(corr_tokens, 10, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0714, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = model.run_with_hooks(\n",
    "        clean_tokens[0], \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(clean_tokens[0], mean_ablate=True)\n",
    "        )\n",
    "print(logit_diff_fn(logits, clean_label_tokens[0], corr_label_tokens[0]))\n",
    "del logits\n",
    "cleanup_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "class KeyboardInterruptBlocker:\n",
    "    def __enter__(self):\n",
    "        # Block SIGINT and store old mask\n",
    "        self.old_mask = signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGINT})\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        # Restore old mask (unblock SIGINT)\n",
    "        signal.pthread_sigmask(signal.SIG_SETMASK, self.old_mask)\n",
    "\n",
    "class Range:\n",
    "    def __init__(self, *args):\n",
    "        # Support for range(start, stop, step) or range(stop)\n",
    "        self.args = args\n",
    "\n",
    "        # Validate input like the built-in range does\n",
    "        if len(self.args) not in {1, 2, 3}:\n",
    "            raise TypeError(f\"Range expected at most 3 arguments, got {len(self.args)}\")\n",
    "        \n",
    "        self.range = __builtins__.range(*self.args)  # Create the range object\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in self.range:\n",
    "            try:\n",
    "                with KeyboardInterruptBlocker():\n",
    "                    yield i\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Keyboard interrupt received. Exiting iteration.\")\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "def forward_pass(batch, labels, logitfn, ratio_trained=1):\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    tokens = batch\n",
    "    logits = logitfn(tokens)\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    loss = F.cross_entropy(last_token_logits, labels)\n",
    "    sparsity_loss = 0\n",
    "    for sae in saes:\n",
    "        sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "    \n",
    "    sparsity_loss = sparsity_loss / len(saes)\n",
    "    return loss, sparsity_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100000*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens.shape[0]*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training_run(token_dataset, labels_dataset, sparsity_multiplier, example_length=6, loss_function='ce', per_token_mask=False, use_mask=False, mean_mask=False, distinct_sparsity_multiplier=0):\n",
    "\n",
    "    def logitfn(tokens):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask)\n",
    "            )\n",
    "        return logits\n",
    "\n",
    "    def forward_pass(batch, labels, logitfn, ratio_trained=1):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        tokens = batch\n",
    "        logits = logitfn(tokens)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        loss = F.cross_entropy(last_token_logits, labels)\n",
    "\n",
    "        sparsity_loss = 0\n",
    "        if per_token_mask:\n",
    "            distinct_sparsity_loss = 0\n",
    "        for sae in saes:\n",
    "            sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "            if per_token_mask:\n",
    "                distinct_sparsity_loss = distinct_sparsity_loss + sae.mask.distinct_sparsity_loss\n",
    "        \n",
    "        sparsity_loss = sparsity_loss / len(saes)\n",
    "        distinct_sparsity_loss = distinct_sparsity_loss / len(saes)\n",
    "\n",
    "        return loss, sparsity_loss, distinct_sparsity_loss\n",
    "\n",
    "    print(\"doing a run with sparsity multiplier\", sparsity_multiplier)\n",
    "    all_optimized_params = []\n",
    "    config = {\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"total_steps\": token_dataset.shape[0]*0.5,\n",
    "        \"sparsity_multiplier\": sparsity_multiplier\n",
    "    }\n",
    "\n",
    "    for sae in saes:\n",
    "        if per_token_mask:\n",
    "            sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length, distinct_l1=1.0).to(device)\n",
    "        else:\n",
    "            sae.mask = SparseMask(sae.cfg.d_sae, 1.0).to(device)\n",
    "        all_optimized_params.extend(list(sae.mask.parameters()))\n",
    "        sae.mask.max_temp = torch.tensor(200.0)\n",
    "    \n",
    "    wandb.init(project=\"sae circuits\", config=config)\n",
    "    optimizer = optim.Adam(all_optimized_params, lr=config[\"learning_rate\"])\n",
    "    total_steps = config[\"total_steps\"] #*config[\"batch_size\"]\n",
    "\n",
    "    with tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n",
    "        for i, (x, y) in enumerate(zip(token_dataset, labels_dataset)):\n",
    "            with KeyboardInterruptBlocker():\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Calculate ratio trained\n",
    "                ratio_trained = i / total_steps\n",
    "                \n",
    "                # Update mask ratio for each SAE\n",
    "                for sae in saes:\n",
    "                    sae.mask.ratio_trained = ratio_trained\n",
    "                \n",
    "                # Forward pass with updated ratio_trained\n",
    "                loss, sparsity_loss, distinct_sparsity_loss = forward_pass(x, y, logitfn, ratio_trained=ratio_trained)\n",
    "                if per_token_mask:\n",
    "                    sparsity_loss = sparsity_loss / example_length\n",
    "\n",
    "                avg_nonzero_elements = sparsity_loss\n",
    "                avg_distinct_nonzero_elements = distinct_sparsity_loss\n",
    "                    \n",
    "                sparsity_loss = sparsity_loss * config[\"sparsity_multiplier\"] + distinct_sparsity_loss * distinct_sparsity_multiplier\n",
    "                total_loss = loss + sparsity_loss\n",
    "                infodict  = {\"Step\": i, \"Progress\": ratio_trained, \"Avg Nonzero Elements\": avg_nonzero_elements.item(), \"avg distinct lat/sae\":avg_distinct_nonzero_elements.item(), \"Task Loss\": loss.item(), \"Sparsity Loss\": sparsity_loss.item(), \"temperature\": saes[0].mask.temperature}\n",
    "                wandb.log(infodict)\n",
    "                \n",
    "                # Backward pass and optimizer step\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update tqdm bar with relevant metrics\n",
    "                pbar.set_postfix(infodict)\n",
    "                \n",
    "                # Update the tqdm progress bar\n",
    "                pbar.update(1)\n",
    "                if i >= total_steps*1.3:\n",
    "                    break\n",
    "    wandb.finish()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for sae in saes:\n",
    "        for param in sae.parameters():\n",
    "            param.grad = None\n",
    "        for param in sae.mask.parameters():\n",
    "            param.grad = None\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    mask_dict = {}\n",
    "\n",
    "    total_density = 0\n",
    "    for sae in saes:\n",
    "        mask_dict[sae.cfg.hook_name] = torch.where(torch.sigmoid(sae.mask.mask*10000))[0].tolist()   # rob thinks .view(-1) needed here\n",
    "        total_density += torch.sigmoid(sae.mask.mask*10000).sum().item()\n",
    "    mask_dict[\"total_density\"] = total_density\n",
    "    mask_dict['avg_density'] = total_density / len(saes)\n",
    "\n",
    "    if per_token_mask:\n",
    "        print(\"total # latents in circuit: \", total_density)\n",
    "    print(\"avg density\", mask_dict['avg_density'])\n",
    "\n",
    "    ### EVAL ###\n",
    "    def masked_logit_fn(tokens):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask, binarize_mask=True)\n",
    "            )\n",
    "        return logits\n",
    "\n",
    "    def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        tokens = batch\n",
    "        logits = logitfn(tokens)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        loss = F.cross_entropy(last_token_logits, labels)\n",
    "        return loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = eval_ce_loss(token_dataset[-1], labels_dataset[-1], masked_logit_fn)\n",
    "        print(\"CE loss:\", loss)\n",
    "\n",
    "    mask_dict['ce_loss'] = loss.item()\n",
    "\n",
    "\n",
    "    json.dump(mask_dict, open(f\"{str(sparsity_multiplier)}_run.json\", \"w\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE loss: tensor(4.5691, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for sae in saes:\n",
    "    sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=7, distinct_l1=1.0).to(device)\n",
    "\n",
    "def masked_logit_fn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, use_mask=True, mean_mask=True, binarize_mask=True)\n",
    "        )\n",
    "\n",
    "def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    tokens = batch\n",
    "    logits = logitfn(tokens)\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    loss = F.cross_entropy(last_token_logits, labels)\n",
    "    return loss\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = eval_ce_loss(clean_tokens[21], clean_label_tokens[21], masked_logit_fn)\n",
    "    print(\"CE loss:\", loss)\n",
    "    cleanup_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing a run with sparsity multiplier 0.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.19.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work/pi_jensen_umass_edu/jnainani_umass_edu/ScalableSAECircuits/wandb/run-20241225_000737-dm3nps7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/dm3nps7a' target=\"_blank\">honest-cherry-6</a></strong> to <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/dm3nps7a' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/dm3nps7a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 408it [02:36,  2.61it/s, Step=407, Progress=1.3, Avg Nonzero Elements=2.68, avg distinct lat/sae=18.5, Task Loss=3.15, Sparsity Loss=0.268, temperature=tensor(992.7993)]                                  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbad6bb179d4650965f7379358de375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Avg Nonzero Elements</td><td>█▇▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Progress</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Sparsity Loss</td><td>█▇▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>Task Loss</td><td>▄▇█▄▇▅▄▄▃▃▆▄▅▃▆▆▅▄▁▃▄▄▄▆▇▅▄▂▂▅▅▂▂▄▃▄▃▃▁▁</td></tr><tr><td>avg distinct lat/sae</td><td>█▇▅▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>temperature</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Avg Nonzero Elements</td><td>2.67857</td></tr><tr><td>Progress</td><td>1.3024</td></tr><tr><td>Sparsity Loss</td><td>0.26786</td></tr><tr><td>Step</td><td>407</td></tr><tr><td>Task Loss</td><td>3.14655</td></tr><tr><td>avg distinct lat/sae</td><td>18.5</td></tr><tr><td>temperature</td><td>992.79932</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">honest-cherry-6</strong> at: <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/dm3nps7a' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits/runs/dm3nps7a</a><br/> View project at: <a href='https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits' target=\"_blank\">https://wandb.ai/jnainani-university-of-massachusetts-amherst/sae%20circuits</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241225_000737-dm3nps7a/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # latents in circuit:  75.0\n",
      "avg density 18.75\n",
      "CE loss: tensor(4.5838, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "do_training_run(clean_tokens, clean_label_tokens, 0.1, example_length=7, loss_function=\"ce\", per_token_mask=True, use_mask=True, mean_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(50, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(saes[3].mask.mask > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blocks.7.hook_resid_post': [9152, 4146, 14287],\n",
       " 'blocks.14.hook_resid_post': [5082, 9224, 11236],\n",
       " 'blocks.21.hook_resid_post': [7957,\n",
       "  13730,\n",
       "  7957,\n",
       "  12577,\n",
       "  170,\n",
       "  209,\n",
       "  865,\n",
       "  3687,\n",
       "  4297,\n",
       "  4714,\n",
       "  5141,\n",
       "  6937,\n",
       "  8735,\n",
       "  9502,\n",
       "  9591,\n",
       "  10849,\n",
       "  11008,\n",
       "  12057,\n",
       "  15950],\n",
       " 'blocks.40.hook_resid_post': [23,\n",
       "  520,\n",
       "  1035,\n",
       "  1155,\n",
       "  1433,\n",
       "  1452,\n",
       "  1743,\n",
       "  1980,\n",
       "  2235,\n",
       "  2285,\n",
       "  2488,\n",
       "  2664,\n",
       "  2693,\n",
       "  2820,\n",
       "  3111,\n",
       "  3510,\n",
       "  3764,\n",
       "  3765,\n",
       "  4330,\n",
       "  4661,\n",
       "  4740,\n",
       "  4843,\n",
       "  6013,\n",
       "  6214,\n",
       "  6426,\n",
       "  6574,\n",
       "  6851,\n",
       "  7776,\n",
       "  7788,\n",
       "  8003,\n",
       "  8502,\n",
       "  8583,\n",
       "  9294,\n",
       "  9696,\n",
       "  10455,\n",
       "  10702,\n",
       "  10941,\n",
       "  11051,\n",
       "  11314,\n",
       "  11654,\n",
       "  12333,\n",
       "  12905,\n",
       "  13582,\n",
       "  13762,\n",
       "  14357,\n",
       "  14988,\n",
       "  15427,\n",
       "  15600,\n",
       "  16048,\n",
       "  16098]}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_mask = {}\n",
    "for sae in saes: \n",
    "    actual_mask[sae.cfg.hook_name] = torch.where(sae.mask.mask > 0)[1].tolist()\n",
    "actual_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the mask\n",
    "json.dump(actual_mask, open(\"updated_01_run.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   23,   520,  1035,  1155,  1433,  1452,  1743,  1980,  2235,  2285,\n",
       "         2488,  2664,  2693,  2820,  3111,  3510,  3764,  3765,  4330,  4661,\n",
       "         4740,  4843,  6013,  6214,  6426,  6574,  6851,  7776,  7788,  8003,\n",
       "         8502,  8583,  9294,  9696, 10455, 10702, 10941, 11051, 11314, 11654,\n",
       "        12333, 12905, 13582, 13762, 14357, 14988, 15427, 15600, 16048, 16098],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(saes[3].mask.mask > 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 latents: 3 🖕\n",
      "layer 1 latents: 3 🖕\n",
      "layer 2 latents: 19 🖕\n",
      "layer 3 latents: 50 🖕\n",
      "total # latents: 75 🖕\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# Assuming 'saes', 'model', and 'simple_dataset' are defined\n",
    "tokens = model.to_str_tokens(clean_tokens[2][0])\n",
    "num_masks = 4 # Number of masks you have\n",
    "counts_per_mask = []\n",
    "for mask_index in range(num_masks):\n",
    "    testmask = saes[mask_index].mask.mask.data.clone()\n",
    "    binarized = (testmask > 0.0).float()\n",
    "    counts = []\n",
    "    for i in range(len(tokens)):\n",
    "        counts.append(torch.count_nonzero(binarized[i]).item())\n",
    "    counts_per_mask.append(counts)\n",
    "\n",
    "# Convert counts to a NumPy array\n",
    "data = np.array(counts_per_mask) # Shape: (num_masks, num_tokens)\n",
    "\n",
    "# Create a mask for zero values\n",
    "zero_mask = data == 0\n",
    "\n",
    "# Define a colormap\n",
    "cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "\n",
    "# Plot the heatmap with the mask\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.heatmap(\n",
    "    data,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap=cmap,\n",
    "    mask=zero_mask,\n",
    "    cbar_kws={'label': 'Counts'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "\n",
    "# Set x-axis labels to tokens\n",
    "ax.set_xticks(np.arange(len(tokens)) + 0.5)\n",
    "ax.set_xticklabels(tokens, rotation=90, fontsize=8)\n",
    "\n",
    "# Add numeric indices above the chart\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks(np.arange(len(tokens)) + 0.5)\n",
    "ax2.set_xticklabels(np.arange(len(tokens)), rotation=90)  # Rotate indices 90 degrees\n",
    "ax2.set_xlabel('Token Indices')\n",
    "\n",
    "# Set y-axis labels to masks\n",
    "ax.set_yticks(np.arange(num_masks) + 0.5)\n",
    "ax.set_yticklabels([f'SAE {i}' for i in range(num_masks)], rotation=0)\n",
    "\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('SAE Number Active Latents')\n",
    "plt.title('Active SAE Latents per Token per Mask (Zero Counts Hidden)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print the total # latents in the circuit\n",
    "total_latents = 0\n",
    "for i, sae in enumerate(saes):\n",
    "    print(f\"layer {i} latents: {torch.sum(sae.mask.mask > 0)} 🖕\")\n",
    "    total_latents += torch.sum(sae.mask.mask > 0)\n",
    "print(f\"total # latents: {total_latents} 🖕\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9152]], device='cuda:0')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero((saes[0].mask.mask>0)[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04241183027625084 0.125\n"
     ]
    }
   ],
   "source": [
    "logits = model(clean_tokens[21])\n",
    "probs = F.softmax(logits[torch.arange(logits.shape[0]), -1], dim=-1)\n",
    "prob_correct = probs[torch.arange(probs.shape[0]), clean_label_tokens[21]].mean().item()\n",
    "max_prob_idxs = probs.argmax(dim=-1)\n",
    "top1_acc = (max_prob_idxs == clean_label_tokens[21]).float().mean().item()\n",
    "print(prob_correct, top1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.054300688207149506 0.1875\n"
     ]
    }
   ],
   "source": [
    "for sae in saes:\n",
    "    sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=7, distinct_l1=1.0).to(device)\n",
    "\n",
    "logits = model.run_with_hooks(\n",
    "        clean_tokens[21], \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(clean_tokens[21], use_mask=False, mean_mask=False) #, binarize_mask=True)\n",
    "        )\n",
    "probs = F.softmax(logits[torch.arange(logits.shape[0]), -1], dim=-1)\n",
    "prob_correct = probs[torch.arange(probs.shape[0]), clean_label_tokens[21]].mean().item()\n",
    "max_prob_idxs = probs.argmax(dim=-1)\n",
    "top1_acc = (max_prob_idxs == clean_label_tokens[21]).float().mean().item()\n",
    "print(prob_correct, top1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_probabilities(logits, labels):\n",
    "    \"\"\"Compute probabilities, logit differences, and CE loss for token predictions.\"\"\"\n",
    "    # Get probabilities for all tokens at the sequence positions\n",
    "    probs = F.softmax(logits[torch.arange(logits.shape[0]), -1], dim=-1)\n",
    "    \n",
    "    # Calculate mean probabilities for correct and error tokens\n",
    "    prob_correct = probs[torch.arange(probs.shape[0]), labels].mean().item()\n",
    "    max_prob_idxs = probs.argmax(dim=-1)\n",
    "    top1_acc = (max_prob_idxs == labels).float().mean().item()\n",
    "    # prob_error = get_highest_other_prob(probs, labels).mean().item()\n",
    "    \n",
    "    # Calculate logit difference\n",
    "    correct_logits = logits[torch.arange(logits.shape[0]), -1, labels]\n",
    "    # error_logits = get_highest_other_logit(logits[torch.arange(logits.shape[0]), -1], labels)\n",
    "    logit_diff = (correct_logits - error_logits).mean().item()\n",
    "    \n",
    "    # Calculate CE loss\n",
    "    ce_loss = F.cross_entropy(\n",
    "        logits[torch.arange(logits.shape[0]), -1], \n",
    "        labels\n",
    "    ).item()\n",
    "    \n",
    "    return prob_correct, prob_error, logit_diff, top1_acc, ce_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

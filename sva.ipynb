{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from functools import partial\n",
    "import einops\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ") \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint as pp\n",
    "from typing import Tuple\n",
    "from torch import Tensor\n",
    "from functools import lru_cache\n",
    "from typing import TypedDict, Optional, Tuple, Union\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24a830f33b6410993c0cab8af7ecf88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-9b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "with open(\"config.json\", 'r') as file:\n",
    "   config = json.load(file)\n",
    "token = config.get('huggingface_token', None)\n",
    "os.environ[\"HF_TOKEN\"] = token\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "hf_cache = \"/work/pi_jensen_umass_edu/jnainani_umass_edu/mechinterp/huggingface_cache/hub\"\n",
    "os.environ[\"HF_HOME\"] = hf_cache\n",
    "\n",
    "# Load the model\n",
    "model = HookedSAETransformer.from_pretrained(\"google/gemma-2-9b\", device=device, cache_dir=hf_cache) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = model.tokenizer.pad_token_id\n",
    "for param in model.parameters():\n",
    "   param.requires_grad_(False)\n",
    "\n",
    "device = \"cuda\"\n",
    "layers= [7, 14, 21, 40]\n",
    "l0s = [92, 67, 129, 125]\n",
    "saes = [SAE.from_pretrained(release=\"gemma-scope-9b-pt-res\", sae_id=f\"layer_{layers[i]}/width_16k/average_l0_{l0s[i]}\", device=device)[0] for i in range(len(layers))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cuda():\n",
    "   torch.cuda.empty_cache()\n",
    "   gc.collect()\n",
    "\n",
    "def clear_memory():\n",
    "   for sae in saes:\n",
    "      for param in sae.parameters():\n",
    "         param.grad = None\n",
    "      for param in sae.mask.parameters():\n",
    "         param.grad = None\n",
    "\n",
    "   for param in model.parameters():\n",
    "      param.grad = None\n",
    "   cleanup_cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMask(nn.Module):\n",
    "    def __init__(self, shape, l1, seq_len=None, distinct_l1=0):\n",
    "        super().__init__()\n",
    "        if seq_len is not None:\n",
    "            self.mask = nn.Parameter(torch.ones(seq_len, shape))\n",
    "        else:\n",
    "            self.mask = nn.Parameter(torch.ones(shape))\n",
    "        self.l1 = l1\n",
    "        self.distinct_l1 = distinct_l1\n",
    "        self.max_temp = torch.tensor(1000.0)\n",
    "        self.sparsity_loss = None\n",
    "        self.ratio_trained = 1\n",
    "        self.temperature = 1\n",
    "        self.distinct_sparsity_loss = 0\n",
    "\n",
    "\n",
    "    def forward(self, x, binary=False, mean_ablation=None):\n",
    "        if binary:\n",
    "            # binary mask, 0 if negative, 1 if positive\n",
    "            binarized = (self.mask > 0).float()\n",
    "            if mean_ablation is None:\n",
    "                return x * binarized\n",
    "            else:\n",
    "                diff = x - mean_ablation\n",
    "                return diff * binarized + mean_ablation\n",
    "            \n",
    "\n",
    "        self.temperature = self.max_temp ** self.ratio_trained\n",
    "        mask = torch.sigmoid(self.mask * self.temperature)\n",
    "        self.sparsity_loss = torch.abs(mask).sum() * self.distinct_l1\n",
    "        if len(mask.shape) == 2:\n",
    "            self.distinct_sparsity_loss = torch.abs(mask).max(dim=0).values.sum() * self.l1\n",
    "\n",
    "        if mean_ablation is None:\n",
    "            return x * mask\n",
    "        else:\n",
    "            diff = x - mean_ablation\n",
    "            return diff * mask + mean_ablation\n",
    "\n",
    "# for sae in saes:\n",
    "#     sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = model.tokenizer.bos_token_id\n",
    "\n",
    "def build_sae_hook_fn(\n",
    "    sae,\n",
    "    sequence,\n",
    "    circuit_mask=None,\n",
    "    use_mask=False,\n",
    "    binarize_mask=False,\n",
    "    mean_mask=False,\n",
    "    ig_mask_threshold=None,\n",
    "    cache_sae_grads=False,\n",
    "    cache_masked_activations=False,\n",
    "    cache_sae_activations=False,\n",
    "    mean_ablate=False, \n",
    "    fake_activations=False):    \n",
    "    \n",
    "    mask = torch.ones_like(sequence, dtype=torch.bool)\n",
    "    mask[sequence == bos_token_id] = False \n",
    "\n",
    "    def sae_hook(value, hook):\n",
    "        # print(f\"sae {sae.cfg.hook_name} running at layer {hook.layer()}\")\n",
    "        feature_acts = sae.encode(value)\n",
    "        feature_acts = feature_acts * mask.unsqueeze(-1)\n",
    "        if fake_activations != False and sae.cfg.hook_layer == fake_activations[0]:\n",
    "            feature_acts = fake_activations[1]\n",
    "        if cache_sae_grads:\n",
    "            raise NotImplementedError(\"torch is confusing\")\n",
    "            sae.feature_acts = feature_acts.requires_grad_(True)\n",
    "            sae.feature_acts.retain_grad()\n",
    "        \n",
    "        if cache_sae_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        \n",
    "        # Learned Binary Masking\n",
    "        if use_mask:\n",
    "            if mean_mask:\n",
    "                # apply the mask, with mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                # apply the mask, without mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask)\n",
    "\n",
    "        # IG Masking\n",
    "        if ig_mask_threshold != None:\n",
    "            # apply the ig mask\n",
    "            if mean_mask:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold)\n",
    "\n",
    "        if circuit_mask is not None:\n",
    "            raise NotImplementedError(\"mask interface not supported\")\n",
    "            mask_method = circuit_mask['mask_method']\n",
    "            mask_indices = circuit_mask[sae.cfg.hook_name]\n",
    "            if mask_method == 'keep_only':\n",
    "                # any activations not in the mask are set to 0\n",
    "                expanded_circuit_mask = torch.zeros_like(feature_acts)\n",
    "                expanded_circuit_mask[:, :, mask_indices] = 1\n",
    "                feature_acts = feature_acts * expanded_circuit_mask\n",
    "            elif mask_method == 'zero_only':\n",
    "                feature_acts[:, :, mask_indices] = 0\n",
    "            else:\n",
    "                raise ValueError(f\"mask_method {mask_method} not recognized\")\n",
    "            \n",
    "        if cache_masked_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        if mean_ablate:\n",
    "            feature_acts = sae.mean_ablation\n",
    "\n",
    "        out = sae.decode(feature_acts)\n",
    "        # choose out or value based on the mask\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(value)\n",
    "        value = torch.where(mask_expanded, out, value)\n",
    "        return value\n",
    "    return sae_hook\n",
    "\n",
    "def build_hooks_list(sequence,\n",
    "                    cache_sae_activations=False,\n",
    "                    cache_sae_grads=False,\n",
    "                    circuit_mask=None,\n",
    "                    use_mask=False,\n",
    "                    binarize_mask=False,\n",
    "                    mean_mask=False,\n",
    "                    cache_masked_activations=False,\n",
    "                    mean_ablate=False,\n",
    "                    fake_activations: Tuple[int, torch.Tensor] = False,\n",
    "                    ig_mask_threshold=None,\n",
    "                    ):\n",
    "    hooks = []\n",
    "    for sae in saes:\n",
    "        hooks.append(\n",
    "            (\n",
    "            sae.cfg.hook_name,\n",
    "            build_sae_hook_fn(sae, sequence, cache_sae_grads=cache_sae_grads, circuit_mask=circuit_mask, use_mask=use_mask, binarize_mask=binarize_mask, cache_masked_activations=cache_masked_activations, cache_sae_activations=cache_sae_activations, mean_mask=mean_mask, mean_ablate=mean_ablate, fake_activations=fake_activations, ig_mask_threshold=ig_mask_threshold),\n",
    "            )\n",
    "        )\n",
    "    return hooks \n",
    "\n",
    "def build_sae_logitfn(**kwargs):\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, **kwargs)\n",
    "            )\n",
    "    return logitfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clean_prefix': 'The friends that the dancer visits', 'patch_prefix': 'The friend that the dancer visits', 'clean_answer': ' go', 'patch_answer': ' goes', 'case': 'plural_singular'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = 'data/sva/rc_train.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "for entry in data:\n",
    "    print(entry)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "corr_data = []\n",
    "clean_labels = []\n",
    "corr_labels = []\n",
    "for entry in data:\n",
    "    if model.to_tokens(entry['clean_prefix']).shape[-1] == 7:\n",
    "        clean_data.append(entry['clean_prefix'])\n",
    "        corr_data.append(entry['patch_prefix'])\n",
    "        clean_labels.append(entry['clean_answer'])\n",
    "        corr_labels.append(entry['patch_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c2c3a00f69417492ae27a5a1bedd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The friend that the dancer visits <u>had just got fired'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"The friend that the dancer visits\", max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'The', ' doctor', ' that', ' the', ' chef', ' for', 'gives']\n",
      "Tokenized answer: [' are']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.82</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.14</span><span style=\"font-weight: bold\">% Token: | are|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m62\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m24.82\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.14\u001b[0m\u001b[1m% Token: | are|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 29.81 Prob: 20.53% Token: | is|\n",
      "Top 1th token. Logit: 28.81 Prob:  7.56% Token: |,|\n",
      "Top 2th token. Logit: 28.64 Prob:  6.40% Token: | and|\n",
      "Top 3th token. Logit: 28.63 Prob:  6.35% Token: | the|\n",
      "Top 4th token. Logit: 28.35 Prob:  4.76% Token: | for|\n",
      "Top 5th token. Logit: 28.29 Prob:  4.49% Token: | him|\n",
      "Top 6th token. Logit: 28.16 Prob:  3.96% Token: |.|\n",
      "Top 7th token. Logit: 27.99 Prob:  3.33% Token: | his|\n",
      "Top 8th token. Logit: 27.98 Prob:  3.30% Token: | in|\n",
      "Top 9th token. Logit: 27.88 Prob:  3.00% Token: | himself|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' are'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' are'\u001b[0m, \u001b[1;36m62\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "test_prompt(\"The doctor that the chef forgives\", \" are\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_tokens(entry['clean_prefix']).shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The carpenters that the dancers praise'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_tokens(entry['clean_prefix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 7]) torch.Size([10, 7])\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "clean_tokens = model.to_tokens(clean_data[:N])\n",
    "corr_tokens = model.to_tokens(corr_data[:N])\n",
    "clean_label_tokens = model.to_tokens(clean_labels[:N], prepend_bos=False).squeeze(-1)\n",
    "corr_label_tokens = model.to_tokens(corr_labels[:N], prepend_bos=False).squeeze(-1)\n",
    "print(clean_tokens.shape, corr_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mask = False \n",
    "mean_mask = False\n",
    "logits = model.run_with_hooks(\n",
    "    clean_tokens, \n",
    "    return_type=\"logits\", \n",
    "    fwd_hooks=build_hooks_list(clean_tokens, use_mask=use_mask, mean_mask=mean_mask)\n",
    "    )\n",
    "def logit_diff_fn(logits, clean_labels, corr_labels, token_wise=False):\n",
    "    clean_logits = logits[torch.arange(logits.shape[0]), -1, clean_labels]\n",
    "    corr_logits = logits[torch.arange(logits.shape[0]), -1, corr_labels]\n",
    "    return (clean_logits - corr_logits).mean() if not token_wise else (clean_logits - corr_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8465, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_diff_fn(logits, clean_label_tokens, corr_label_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def memoize_encode(text):\n",
    "    return(model.to_tokens(text))\n",
    "\n",
    "def response_to_token(response):\n",
    "    return memoize_encode(str(response)).squeeze()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset = []\n",
    "simple_labels = []\n",
    "\n",
    "# answer_token = model.to_single_token(\"1\")\n",
    "# traceback_token = model.to_single_token(\"Traceback\")\n",
    "\n",
    "for item in json_dataset:\n",
    "    simple_dataset.append(item[\"correct\"][\"prompt\"])\n",
    "    simple_dataset.append(item[\"error\"][\"prompt\"])\n",
    "    simple_labels.append(response_to_token(item[\"correct\"][\"response\"]))\n",
    "    simple_labels.append(response_to_token(item[\"error\"][\"response\"]))\n",
    "\n",
    "\n",
    "simple_dataset = model.to_tokens(simple_dataset)\n",
    "simple_labels = torch.tensor(simple_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = simple_labels.view(-1).unique()\n",
    "\n",
    "def get_highest_other_logit(logits, label):\n",
    "    other_logits = logits.clone()\n",
    "    # set the logits for the correct label to -infinity\n",
    "    other_logits[..., label] = -float(\"inf\")\n",
    "    # get the highest logit for the other labels\n",
    "    mask = torch.ones_like(other_logits) * -float(\"inf\")\n",
    "    mask[..., vocab] = 0\n",
    "    other_logits = other_logits + mask\n",
    "\n",
    "    return other_logits.max(dim=-1).values\n",
    "\n",
    "def get_highest_other_prob(logits, label):\n",
    "    other_logits = logits.clone()\n",
    "    # set the logits for the correct label to -infinity\n",
    "    other_logits[..., label] = 0\n",
    "    # get the highest logit for the other labels\n",
    "    #mask = torch.ones_like(other_logits) * 0\n",
    "    #mask[..., vocab] = 0\n",
    "    #other_logits = other_logits + mask\n",
    "\n",
    "    return other_logits.max(dim=-1).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_token_probabilities(logits, seq_idxs, labels):\n",
    "    \"\"\"Compute probabilities, logit differences, and CE loss for token predictions.\"\"\"\n",
    "    # Get probabilities for all tokens at the sequence positions\n",
    "    probs = F.softmax(logits[torch.arange(logits.shape[0]), seq_idxs], dim=-1)\n",
    "    \n",
    "    # Calculate mean probabilities for correct and error tokens\n",
    "    prob_correct = probs[torch.arange(probs.shape[0]), labels].mean().item()\n",
    "    max_prob_idxs = probs.argmax(dim=-1)\n",
    "    top1_acc = (max_prob_idxs == labels).float().mean().item()\n",
    "    prob_error = get_highest_other_prob(probs, labels).mean().item()\n",
    "    \n",
    "    # Calculate logit difference\n",
    "    correct_logits = logits[torch.arange(logits.shape[0]), seq_idxs, labels]\n",
    "    error_logits = get_highest_other_logit(logits[torch.arange(logits.shape[0]), seq_idxs], labels)\n",
    "    logit_diff = (correct_logits - error_logits).mean().item()\n",
    "    \n",
    "    # Calculate CE loss\n",
    "    ce_loss = F.cross_entropy(\n",
    "        logits[torch.arange(logits.shape[0]), seq_idxs], \n",
    "        labels\n",
    "    ).item()\n",
    "    \n",
    "    return prob_correct, prob_error, logit_diff, top1_acc, ce_loss\n",
    "\n",
    "def sanity_check_model_performance(logitfn):\n",
    "    \"\"\"Run sanity checks on model performance using contrastive examples.\"\"\"\n",
    "    baseline_dataset = ContrastiveDatasetBatch(json_dataset[-10:])\n",
    "    \n",
    "    # Get logits for both correct and error examples\n",
    "    correct_logits = logitfn(baseline_dataset.correct_tokenized)\n",
    "    error_logits = logitfn(baseline_dataset.error_tokenized)\n",
    "    \n",
    "    # Analyze correct examples\n",
    "    prob_correct_in_correct, prob_error_in_correct, logit_diff_correct, top1_correct, ce_loss_correct = compute_token_probabilities(\n",
    "        correct_logits,\n",
    "        baseline_dataset.correct_answer_seq_idxs,\n",
    "        baseline_dataset.correct_labels,\n",
    "    )\n",
    "    \n",
    "    # Analyze error examples\n",
    "    prob_error_in_error, prob_correct_in_error, logit_diff_error, top1_error, ce_loss_error = compute_token_probabilities(\n",
    "        error_logits,\n",
    "        baseline_dataset.error_answer_seq_idxs,\n",
    "        baseline_dataset.error_labels,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'prob_age_given_correct': prob_correct_in_correct,\n",
    "        'prob_traceback_given_correct(bad)': prob_error_in_correct,\n",
    "        'logit_diff_correct(-=bad)': logit_diff_correct,\n",
    "        'top1_correct': top1_correct,\n",
    "        'ce_loss_correct': ce_loss_correct,\n",
    "        'prob_traceback_given_error': prob_error_in_error,\n",
    "        'prob_age_given_error(bad)': prob_correct_in_error,\n",
    "        'logit_diff_error(-=bad)': logit_diff_error,\n",
    "        'top1_error': top1_error,\n",
    "        'ce_loss_error': ce_loss_error\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bos_token_id = model.tokenizer.bos_token_id\n",
    "\n",
    "def build_sae_hook_fn(\n",
    "    # Core components\n",
    "    sae,\n",
    "    sequence,\n",
    "    \n",
    "    # Masking options\n",
    "    circuit_mask=None,\n",
    "    use_mask=False,\n",
    "    binarize_mask=False,\n",
    "    mean_mask=False,\n",
    "    ig_mask_threshold=None,\n",
    "    \n",
    "    # Caching behavior\n",
    "    cache_sae_grads=False,\n",
    "    cache_masked_activations=False,\n",
    "    cache_sae_activations=False,\n",
    "    \n",
    "    # Ablation options\n",
    "    mean_ablate=False,  # Controls mean ablation of the SAE\n",
    "    fake_activations=False,  # Controls whether to use fake activations\n",
    "    ):    # make the mask for the sequence\n",
    "    mask = torch.ones_like(sequence, dtype=torch.bool)\n",
    "    # mask[sequence == pad_token_id] = False\n",
    "    mask[sequence == bos_token_id] = False # where mask is false, keep original\n",
    "    def sae_hook(value, hook):\n",
    "        # print(f\"sae {sae.cfg.hook_name} running at layer {hook.layer()}\")\n",
    "        feature_acts = sae.encode(value)\n",
    "        feature_acts = feature_acts * mask.unsqueeze(-1)\n",
    "        if fake_activations != False and sae.cfg.hook_layer == fake_activations[0]:\n",
    "            feature_acts = fake_activations[1]\n",
    "        if cache_sae_grads:\n",
    "            raise NotImplementedError(\"torch is confusing\")\n",
    "            sae.feature_acts = feature_acts.requires_grad_(True)\n",
    "            sae.feature_acts.retain_grad()\n",
    "        \n",
    "        if cache_sae_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        \n",
    "        # Learned Binary Masking\n",
    "        if use_mask:\n",
    "            if mean_mask:\n",
    "                # apply the mask, with mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                # apply the mask, without mean ablations\n",
    "                feature_acts = sae.mask(feature_acts, binary=binarize_mask)\n",
    "\n",
    "        # IG Masking\n",
    "        if ig_mask_threshold != None:\n",
    "            # apply the ig mask\n",
    "            if mean_mask:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold, mean_ablation=sae.mean_ablation)\n",
    "            else:\n",
    "                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold)\n",
    "\n",
    "                \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        if circuit_mask is not None:\n",
    "            raise NotImplementedError(\"mask interface not supported\")\n",
    "            mask_method = circuit_mask['mask_method']\n",
    "            mask_indices = circuit_mask[sae.cfg.hook_name]\n",
    "            if mask_method == 'keep_only':\n",
    "                # any activations not in the mask are set to 0\n",
    "                expanded_circuit_mask = torch.zeros_like(feature_acts)\n",
    "                expanded_circuit_mask[:, :, mask_indices] = 1\n",
    "                feature_acts = feature_acts * expanded_circuit_mask\n",
    "            elif mask_method == 'zero_only':\n",
    "                feature_acts[:, :, mask_indices] = 0\n",
    "            else:\n",
    "                raise ValueError(f\"mask_method {mask_method} not recognized\")\n",
    "            \n",
    "        if cache_masked_activations:\n",
    "            sae.feature_acts = feature_acts.detach().clone()\n",
    "        if mean_ablate:\n",
    "            feature_acts = sae.mean_ablation\n",
    "\n",
    "        out = sae.decode(feature_acts)\n",
    "        # choose out or value based on the mask\n",
    "        mask_expanded = mask.unsqueeze(-1).expand_as(value)\n",
    "        value = torch.where(mask_expanded, out, value)\n",
    "        return value\n",
    "    return sae_hook\n",
    "\n",
    "\n",
    "    # def sae_hook_ablate(value, hook):\n",
    "    # feature_acts = sae.encode(value)\n",
    "    # # feature_acts[:, :, topsae_attr_indices] = 0\n",
    "    # out = sae.decode(feature_acts)\n",
    "    # return out\n",
    "\n",
    "\n",
    "def build_hooks_list(sequence,\n",
    "                    cache_sae_activations=False,\n",
    "                    cache_sae_grads=False,\n",
    "                    circuit_mask=None,\n",
    "                    use_mask=False,\n",
    "                    binarize_mask=False,\n",
    "                    mean_mask=False,\n",
    "                    cache_masked_activations=False,\n",
    "                    mean_ablate=False,\n",
    "                    fake_activations: Tuple[int, torch.Tensor] = False,\n",
    "                    ig_mask_threshold=None,\n",
    "                    ):\n",
    "    hooks = []\n",
    "    # blocks.0.hook_resid_pre\n",
    "    # # fake hook that adds zero so gradients propagate through the model\n",
    "    # param = nn.Parameter(torch.tensor(0.0, requires_grad=True))\n",
    "    # hooks.append(\n",
    "    #     (\n",
    "    #         \"blocks.0.hook_resid_pre\",\n",
    "    #         lambda value, hook: value + param,\n",
    "    #     )\n",
    "    # )\n",
    "    for sae in saes:\n",
    "        hooks.append(\n",
    "            (\n",
    "            sae.cfg.hook_name,\n",
    "            build_sae_hook_fn(sae, sequence, cache_sae_grads=cache_sae_grads, circuit_mask=circuit_mask, use_mask=use_mask, binarize_mask=binarize_mask, cache_masked_activations=cache_masked_activations, cache_sae_activations=cache_sae_activations, mean_mask=mean_mask, mean_ablate=mean_ablate, fake_activations=fake_activations, ig_mask_threshold=ig_mask_threshold),\n",
    "            )\n",
    "        )\n",
    "    return hooks \n",
    "\n",
    "def build_sae_logitfn(**kwargs):\n",
    "    def logitfn(tokens):\n",
    "        return model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, **kwargs)\n",
    "            )\n",
    "    return logitfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# makethe length of the dataset a multiple of the batch size\n",
    "simple_dataset = simple_dataset[:batch_size*(len(simple_dataset)//batch_size)]\n",
    "simple_labels = simple_labels[:batch_size*(len(simple_labels)//batch_size)]\n",
    "\n",
    "simple_dataset = simple_dataset.view(-1, batch_size, 65)\n",
    "simple_labels = simple_labels.view(-1, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitfn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, use_mask=True, mean_mask=True)\n",
    "        )\n",
    "\n",
    "\n",
    "def forward_pass(batch, labels, logitfn, ratio_trained=1):\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    tokens = batch\n",
    "    logits = logitfn(tokens)\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    loss = F.cross_entropy(last_token_logits, labels)\n",
    "    sparsity_loss = 0\n",
    "    for sae in saes:\n",
    "        sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "    \n",
    "    sparsity_loss = sparsity_loss / len(saes)\n",
    "\n",
    "    return loss, sparsity_loss\n",
    "\n",
    "def mask_logits(logits):\n",
    "    \"\"\"\n",
    "    Mask logits to only allow tokens in vocab_tensor by setting all other logits to -inf.\n",
    "    Works with any number of leading batch dimensions.\n",
    "    \n",
    "    Args:\n",
    "        logits: Tensor of shape [..., vocab_size] containing the logits\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of same shape as logits with all non-vocab logits set to -inf\n",
    "    \"\"\"\n",
    "    vocab_tensor = simple_labels.view(-1).unique()\n",
    "    mask = torch.zeros_like(logits)\n",
    "    mask[..., vocab_tensor] = 1\n",
    "    return logits.masked_fill(mask == 0, float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training_run(sparsity_multiplier, loss_function='ce', per_token_mask=False, use_mask=False, mean_mask=False, distinct_sparsity_multiplier=0):\n",
    "\n",
    "    def logitfn(tokens):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask)\n",
    "            )\n",
    "        if loss_function == 'ce_vocab':\n",
    "            return mask_logits(logits)\n",
    "        return logits\n",
    "\n",
    "    def forward_pass(batch, labels, logitfn, ratio_trained=1):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        tokens = batch\n",
    "        logits = logitfn(tokens)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        if loss_function == 'ce' or loss_function == 'ce_vocab':\n",
    "            loss = F.cross_entropy(last_token_logits, labels)\n",
    "            \n",
    "        elif loss_function == 'logit_diff':\n",
    "            # from the last token logits, get the logit for the \"1\" token and the \"Traceback\" token\n",
    "            correct_logits = last_token_logits[torch.arange(last_token_logits.shape[0]), labels]\n",
    "            incorrect_labels = torch.where(labels == answer_token, traceback_token, answer_token)\n",
    "            incorrect_logits = last_token_logits[torch.arange(last_token_logits.shape[0]), incorrect_labels]\n",
    "            loss = (incorrect_logits - correct_logits).mean() # it should very negative if the model is right.\n",
    "        else:\n",
    "            raise ValueError(f\"Loss function {loss_function} not recognized\")\n",
    "        sparsity_loss = 0\n",
    "        if per_token_mask:\n",
    "            distinct_sparsity_loss = 0\n",
    "        for sae in saes:\n",
    "            sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "            if per_token_mask:\n",
    "                distinct_sparsity_loss = distinct_sparsity_loss + sae.mask.distinct_sparsity_loss\n",
    "        \n",
    "        sparsity_loss = sparsity_loss / len(saes)\n",
    "        distinct_sparsity_loss = distinct_sparsity_loss / len(saes)\n",
    "\n",
    "        return loss, sparsity_loss, distinct_sparsity_loss\n",
    "\n",
    "    print(\"doing a run with sparsity multiplier\", sparsity_multiplier)\n",
    "    all_optimized_params = []\n",
    "    config = {\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"total_steps\": simple_dataset.shape[0]*0.01,\n",
    "        \"sparsity_multiplier\": sparsity_multiplier\n",
    "    }\n",
    "\n",
    "    for sae in saes:\n",
    "        if per_token_mask:\n",
    "            sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=65, distinct_l1=1.0)\n",
    "        else:\n",
    "            sae.mask = SparseMask(sae.cfg.d_sae, 1.0)\n",
    "        all_optimized_params.extend(list(sae.mask.parameters()))\n",
    "        sae.mask.max_temp = torch.tensor(200.0)\n",
    "    \n",
    "    wandb.init(project=\"sae circuits\", config=config)\n",
    "    optimizer = optim.Adam(all_optimized_params, lr=config[\"learning_rate\"])\n",
    "    total_steps = config[\"total_steps\"]\n",
    "\n",
    "    with tqdm(total=total_steps, desc=\"Training Progress\") as pbar:\n",
    "        for i, (x, y) in enumerate(zip(simple_dataset, simple_labels)):\n",
    "            with KeyboardInterruptBlocker():\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Calculate ratio trained\n",
    "                ratio_trained = i / total_steps\n",
    "                \n",
    "                # Update mask ratio for each SAE\n",
    "                for sae in saes:\n",
    "                    sae.mask.ratio_trained = ratio_trained\n",
    "                \n",
    "                # Forward pass with updated ratio_trained\n",
    "                loss, sparsity_loss, distinct_sparsity_loss = forward_pass(x, y, logitfn, ratio_trained=ratio_trained)\n",
    "                if per_token_mask:\n",
    "                    sparsity_loss = sparsity_loss / 65\n",
    "\n",
    "\n",
    "                avg_nonzero_elements = sparsity_loss\n",
    "                avg_distinct_nonzero_elements = distinct_sparsity_loss\n",
    "                    \n",
    "                sparsity_loss = sparsity_loss * config[\"sparsity_multiplier\"] + distinct_sparsity_loss * distinct_sparsity_multiplier\n",
    "                total_loss = loss + sparsity_loss\n",
    "                infodict  = {\"Step\": i, \"Progress\": ratio_trained, \"Avg Nonzero Elements\": avg_nonzero_elements.item(), \"avg distinct lat/sae\":avg_distinct_nonzero_elements.item(), \"Task Loss\": loss.item(), \"Sparsity Loss\": sparsity_loss.item(), \"temperature\": saes[0].mask.temperature}\n",
    "                wandb.log(infodict)\n",
    "                \n",
    "                # Backward pass and optimizer step\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update tqdm bar with relevant metrics\n",
    "                pbar.set_postfix(infodict)\n",
    "                \n",
    "                # Update the tqdm progress bar\n",
    "                pbar.update(1)\n",
    "                if i >= total_steps*1.3:\n",
    "                    break\n",
    "    wandb.finish()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for sae in saes:\n",
    "        for param in sae.parameters():\n",
    "            param.grad = None\n",
    "        for param in sae.mask.parameters():\n",
    "            param.grad = None\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.grad = None\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    mask_dict = {}\n",
    "\n",
    "    total_density = 0\n",
    "    for sae in saes:\n",
    "        mask_dict[sae.cfg.hook_name] = torch.where(torch.sigmoid(sae.mask.mask*10000))[0].tolist()   # rob thinks .view(-1) needed here\n",
    "        total_density += torch.sigmoid(sae.mask.mask*10000).sum().item()\n",
    "    mask_dict[\"total_density\"] = total_density\n",
    "    mask_dict['avg_density'] = total_density / len(saes)\n",
    "\n",
    "    if per_token_mask:\n",
    "        print(\"total # latents in circuit: \", total_density)\n",
    "    print(\"avg density\", mask_dict['avg_density'])\n",
    "\n",
    "    ### EVAL ###\n",
    "    def masked_logit_fn(tokens):\n",
    "        logits =  model.run_with_hooks(\n",
    "            tokens, \n",
    "            return_type=\"logits\", \n",
    "            fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask, binarize_mask=True)\n",
    "            )\n",
    "        if loss_function == 'ce_vocab':\n",
    "            return mask_logits(logits)\n",
    "        return logits\n",
    "\n",
    "    def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):\n",
    "        for sae in saes:\n",
    "            sae.mask.ratio_trained = ratio_trained\n",
    "        tokens = batch\n",
    "        logits = logitfn(tokens)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        loss = F.cross_entropy(last_token_logits, labels)\n",
    "\n",
    "        # sparsity_loss = 0\n",
    "        # for sae in saes:\n",
    "        #     sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "        \n",
    "        # sparsity_loss = sparsity_loss / len(saes)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss = eval_ce_loss(simple_dataset[-1], simple_labels[-1], masked_logit_fn)\n",
    "        print(\"CE loss:\", loss)\n",
    "\n",
    "    mask_dict['ce_loss'] = loss.item()\n",
    "\n",
    "\n",
    "    json.dump(mask_dict, open(f\"{str(sparsity_multiplier)}_run.json\", \"w\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_logit_fn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, use_mask=True, mean_mask=True, binarize_mask=True)\n",
    "        )\n",
    "\n",
    "def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    tokens = batch\n",
    "    logits = logitfn(tokens)\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    loss = F.cross_entropy(last_token_logits, labels)\n",
    "\n",
    "    # sparsity_loss = 0\n",
    "    # for sae in saes:\n",
    "    #     sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "    \n",
    "    # sparsity_loss = sparsity_loss / len(saes)\n",
    "\n",
    "    return loss\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = eval_ce_loss(simple_dataset[21], simple_labels[21], masked_logit_fn)\n",
    "    \n",
    "    print(\"CE loss:\", loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_training_run(14, loss_function='logit_diff', per_token_mask=True, use_mask=True, mean_mask=True)\n",
    "# do_training_run(0.1, loss_function=\"ce\", per_token_mask=True, use_mask=True, mean_mask=True, distinct_sparsity_multiplier=0.02)\n",
    "# do_training_run(0.025, per_token_mask=False, use_mask=True, mean_mask=True)\n",
    "do_training_run(0.6, loss_function=\"ce\", per_token_mask=True, use_mask=True, mean_mask=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitfn(tokens, vocab_mask=False):\n",
    "    logits =  model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, use_mask=True, mean_mask=True, binarize_mask=True)\n",
    "        )\n",
    "    if vocab_mask:\n",
    "        return mask_logits(logits)\n",
    "    return logits\n",
    "\n",
    "with torch.no_grad():\n",
    "    pp(sanity_check_model_performance(logitfn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logitfn(tokens, vocab_mask=False):\n",
    "    logits =  model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, use_mask=True, mean_mask=True, binarize_mask=True)\n",
    "        )\n",
    "    if vocab_mask:\n",
    "        return mask_logits(logits)\n",
    "    return logits\n",
    "\n",
    "# def forward_pass(batch, labels, logitfn, ratio_trained=1):\n",
    "#     for sae in saes:\n",
    "#         sae.mask.ratio_trained = ratio_trained\n",
    "#     tokens = batch\n",
    "#     logits = logitfn(tokens)\n",
    "#     last_token_logits = logits[:, -1, :]\n",
    "#     loss = F.cross_entropy(last_token_logits, labels)\n",
    "#     sparsity_loss = 0\n",
    "#     for sae in saes:\n",
    "#         sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "    \n",
    "#     sparsity_loss = sparsity_loss / len(saes)\n",
    "\n",
    "    return loss, sparsity_loss\n",
    "with torch.no_grad():\n",
    "    idx = 0\n",
    "    print(\"original example\")\n",
    "    print(f\"{model.to_string(simple_dataset[-1][idx])}\")\n",
    "    print(\"original label\")\n",
    "    print(model.to_string(simple_labels[-1][idx:idx+1]))\n",
    "    logits = logitfn(simple_dataset[-1][idx:idx+1][-1])[-1][-1]\n",
    "    print(logits.shape)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    print(probs.shape)\n",
    "    # get the top 3 tokens and their probabilities\n",
    "    topk = torch.topk(probs, k=3)\n",
    "    print(model.to_str_tokens(topk.indices))\n",
    "    print(topk.values)\n",
    "    # get the cross entropy loss\n",
    "    loss = F.cross_entropy(logits.unsqueeze(0), simple_labels[-1][idx:idx+1])\n",
    "    print(f\"ce loss {loss}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    num_items = 16\n",
    "    for i in range(num_items):\n",
    "        example = simple_dataset[-2][i]\n",
    "        label = simple_labels[-2][i]\n",
    "        logits = logitfn(example)[-1][-1]\n",
    "        top_logit = torch.argmax(logits)\n",
    "        if top_logit == label:\n",
    "            print(\"correct\", model.to_string(label))\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(\"error\", model.to_string(label))\n",
    "        \n",
    "    print(f\"correct: {correct} out of {num_items}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "# Assuming 'saes', 'model', and 'simple_dataset' are defined\n",
    "tokens = model.to_str_tokens(simple_dataset[2][0])\n",
    "num_masks = 4 # Number of masks you have\n",
    "counts_per_mask = []\n",
    "for mask_index in range(num_masks):\n",
    "    testmask = saes[mask_index].mask.mask.data.clone()\n",
    "    binarized = (testmask > 0.0).float()\n",
    "    counts = []\n",
    "    for i in range(len(tokens)):\n",
    "        counts.append(torch.count_nonzero(binarized[i]).item())\n",
    "    counts_per_mask.append(counts)\n",
    "\n",
    "# Convert counts to a NumPy array\n",
    "data = np.array(counts_per_mask) # Shape: (num_masks, num_tokens)\n",
    "\n",
    "# Create a mask for zero values\n",
    "zero_mask = data == 0\n",
    "\n",
    "# Define a colormap\n",
    "cmap = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "\n",
    "# Plot the heatmap with the mask\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.heatmap(\n",
    "    data,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap=cmap,\n",
    "    mask=zero_mask,\n",
    "    cbar_kws={'label': 'Counts'},\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray'\n",
    ")\n",
    "\n",
    "# Set x-axis labels to tokens\n",
    "ax.set_xticks(np.arange(len(tokens)) + 0.5)\n",
    "ax.set_xticklabels(tokens, rotation=90, fontsize=8)\n",
    "\n",
    "# Add numeric indices above the chart\n",
    "ax2 = ax.twiny()\n",
    "ax2.set_xlim(ax.get_xlim())\n",
    "ax2.set_xticks(np.arange(len(tokens)) + 0.5)\n",
    "ax2.set_xticklabels(np.arange(len(tokens)), rotation=90)  # Rotate indices 90 degrees\n",
    "ax2.set_xlabel('Token Indices')\n",
    "\n",
    "# Set y-axis labels to masks\n",
    "ax.set_yticks(np.arange(num_masks) + 0.5)\n",
    "ax.set_yticklabels([f'SAE {i}' for i in range(num_masks)], rotation=0)\n",
    "\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('SAE Number Active Latents')\n",
    "plt.title('Active SAE Latents per Token per Mask (Zero Counts Hidden)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# print the total # latents in the circuit\n",
    "total_latents = 0\n",
    "for i, sae in enumerate(saes):\n",
    "    print(f\"layer {i} latents: {torch.sum(sae.mask.mask > 0)} ðŸ–•\")\n",
    "    total_latents += torch.sum(sae.mask.mask > 0)\n",
    "print(f\"total # latents: {total_latents} ðŸ–•\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nonzero(saes[2].mask.mask[64]>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.to_string(simple_dataset[0][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_logit_fn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens, use_mask=True, binarize_mask=True, mean_mask=True)\n",
    "        )\n",
    "\n",
    "def baseline_logit_fn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=[]\n",
    "        )\n",
    "\n",
    "def baseline_sae_logit_fn(tokens):\n",
    "    return model.run_with_hooks(\n",
    "        tokens, \n",
    "        return_type=\"logits\", \n",
    "        fwd_hooks=build_hooks_list(tokens)\n",
    "        )\n",
    "\n",
    "# serialize the masks\n",
    "mask_dict = {}\n",
    "for sae in saes:\n",
    "    mask_dict[sae.cfg.hook_name] = torch.where(torch.sigmoid(sae.mask.mask*10000))[0].tolist()\n",
    "\n",
    "json.dump(mask_dict, open(\"mask_dict.json\", \"w\"))\n",
    "len(mask_dict[\"blocks.7.hook_resid_post\"])\n",
    "\n",
    "for sae in saes:\n",
    "    mask = sae.mask.mask\n",
    "    print(f\"Nonzero elements in mask for {sae.cfg.hook_name}: {torch.count_nonzero(torch.sigmoid(mask*1000))}\")\n",
    "\n",
    "def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    tokens = batch\n",
    "    logits = logitfn(tokens)\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    loss = F.cross_entropy(last_token_logits, labels)\n",
    "    sparsity_loss = 0\n",
    "    for sae in saes:\n",
    "        sparsity_loss = sparsity_loss + sae.mask.sparsity_loss\n",
    "    \n",
    "    sparsity_loss = sparsity_loss / len(saes)\n",
    "\n",
    "    return loss\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = eval_ce_loss(simple_dataset[-1], simple_labels[-1], baseline_logit_fn)\n",
    "    print(\"baseline loss:\", loss)\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = eval_ce_loss(simple_dataset[-1], simple_labels[-1], baseline_sae_logit_fn)\n",
    "    print(\"sae loss: \", loss)\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = eval_ce_loss(simple_dataset[-1], simple_labels[-1], masked_logit_fn)\n",
    "    print(\"ablated loss: \", loss)\n",
    "\n",
    "def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):\n",
    "    # Assuming 'saes' is defined elsewhere in your code\n",
    "    for sae in saes:\n",
    "        sae.mask.ratio_trained = ratio_trained\n",
    "    tokens = batch\n",
    "    logits = logitfn(tokens)\n",
    "    last_token_logits = logits[:, -1, :]\n",
    "    loss = F.cross_entropy(last_token_logits, labels)\n",
    "    sparsity_loss = 0\n",
    "    for sae in saes:\n",
    "        sparsity_loss += sae.mask.sparsity_loss\n",
    "    \n",
    "    sparsity_loss = sparsity_loss / len(saes)\n",
    "    total_loss = loss + sparsity_loss\n",
    "\n",
    "    return loss.item()  # Return the loss as a scalar value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "# Method: VANILLA GEMMA 9B\n",
    "method_name = \"VANILLA GEMMA 9B\"\n",
    "with torch.no_grad():\n",
    "    results = sanity_check_model_performance(baseline_logit_fn)\n",
    "    # Compute cross-entropy loss\n",
    "    ce_loss = eval_ce_loss(simple_dataset[-1], simple_labels[-1], baseline_logit_fn)\n",
    "    results['ce_loss'] = ce_loss\n",
    "    results['method'] = method_name\n",
    "    results_list.append(results)\n",
    "\n",
    "# Method: GEMMA 9B WITH SAE (no masks)\n",
    "method_name = \"GEMMA 9B WITH SAE (no masks)\"\n",
    "with torch.no_grad():\n",
    "    results = sanity_check_model_performance(baseline_sae_logit_fn)\n",
    "    # Compute cross-entropy loss\n",
    "    ce_loss = eval_ce_loss(simple_dataset[-1], simple_labels[-1], baseline_sae_logit_fn)\n",
    "    results['ce_loss'] = ce_loss\n",
    "    results['method'] = method_name\n",
    "    results_list.append(results)\n",
    "\n",
    "# Method: GEMMA 9B WITH SAE Masked\n",
    "method_name = \"GEMMA 9B WITH SAE Masked\"\n",
    "with torch.no_grad():\n",
    "    results = sanity_check_model_performance(masked_logit_fn)\n",
    "    # Compute cross-entropy loss\n",
    "    ce_loss = eval_ce_loss(simple_dataset[-1], simple_labels[-1], masked_logit_fn)\n",
    "    results['ce_loss'] = ce_loss\n",
    "    results['method'] = method_name\n",
    "    results_list.append(results)\n",
    "\n",
    "# Create a DataFrame and display it with descriptive column names\n",
    "df = pd.DataFrame(results_list)\n",
    "df = df.set_index('method')  # Set 'method' as the index\n",
    "\n",
    "# Rename columns for better readability\n",
    "df = df.rename(columns={\n",
    "    'prob_correct_in_correct': 'P(Age|Correct Ex)',\n",
    "    'prob_error_in_correct': 'P(Traceback|Correct Ex)',\n",
    "    'logit_diff_correct': 'Logit Diff (Correct Ex)',\n",
    "    'prob_error_in_error': 'P(Traceback|Error Ex)',\n",
    "    'prob_correct_in_error': 'P(Age|Error Ex)',\n",
    "    'logit_diff_error': 'Logit Diff (Error Ex)',\n",
    "    'ce_loss': 'Cross-Entropy Loss'\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "df\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
